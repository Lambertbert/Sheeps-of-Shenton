{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.54)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: click in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install praw nltk pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\notjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\notjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching AAPL data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching AAPL data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching AAPL data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching AAPL data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching AAPL data from 2024-07-02 to 2024-08-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching AAPL data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching AAPL data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Fetching GOOG data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching GOOG data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching GOOG data from 2024-01-01 to 2024-03-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GOOG data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching GOOG data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching GOOG data from 2024-07-02 to 2024-08-31 with 1h interval...\n",
      "Fetching GOOG data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching GOOG data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching GOOG data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Fetching MSFT data from 2023-09-01 to 2023-10-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching MSFT data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-07-02 to 2024-08-31 with 1h interval...\n",
      "Fetching MSFT data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching MSFT data from 2024-11-01 to 2024-12-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Fetching AMZN data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching AMZN data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching AMZN data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching AMZN data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching AMZN data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching AMZN data from 2024-07-02 to 2024-08-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AMZN data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching AMZN data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching AMZN data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Fetching NVDA data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching NVDA data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching NVDA data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching NVDA data from 2024-03-02 to 2024-05-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NVDA data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching NVDA data from 2024-07-02 to 2024-08-31 with 1h interval...\n",
      "Fetching NVDA data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching NVDA data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching NVDA data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "[DEBUG] AAPL stock data shape: (2462, 7), columns: ['Close_AAPL', 'High_AAPL', 'Low_AAPL', 'Open_AAPL', 'Volume_AAPL', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for AAPL in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 200 posts for AAPL.\n",
      "[DEBUG] sentiment_daily shape: (177, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for AAPL ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2462 entries, 2023-09-01 13:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Close_AAPL     2462 non-null   float64\n",
      " 1   High_AAPL      2462 non-null   float64\n",
      " 2   Low_AAPL       2462 non-null   float64\n",
      " 3   Open_AAPL      2462 non-null   float64\n",
      " 4   Volume_AAPL    2462 non-null   int64  \n",
      " 5   Date           2462 non-null   object \n",
      " 6   avg_sentiment  2462 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 153.9+ KB\n",
      "None\n",
      "                           Close_AAPL   High_AAPL    Low_AAPL   Open_AAPL  \\\n",
      "Datetime                                                                    \n",
      "2023-09-01 13:30:00+00:00  189.279999  189.919998  188.279999  189.485001   \n",
      "2023-09-01 14:30:00+00:00  188.660004  189.279999  188.311005  189.270004   \n",
      "2023-09-01 15:30:00+00:00  189.115005  189.210007  188.360107  188.679993   \n",
      "2023-09-01 16:30:00+00:00  189.000000  189.320007  188.839996  189.110001   \n",
      "2023-09-01 17:30:00+00:00  188.720093  189.225006  188.630005  188.990005   \n",
      "\n",
      "                           Volume_AAPL        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2023-09-01 13:30:00+00:00     12242102  2023-09-01            0.0  \n",
      "2023-09-01 14:30:00+00:00      5217745  2023-09-01            0.0  \n",
      "2023-09-01 15:30:00+00:00      4230174  2023-09-01            0.0  \n",
      "2023-09-01 16:30:00+00:00      3233746  2023-09-01            0.0  \n",
      "2023-09-01 17:30:00+00:00      3052530  2023-09-01            0.0  \n",
      "                           Close_AAPL   High_AAPL    Low_AAPL   Open_AAPL  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  228.400604  229.419998  228.195007  229.085007   \n",
      "2025-02-07 17:30:00+00:00  229.298904  229.440002  228.363297  228.389999   \n",
      "2025-02-07 18:30:00+00:00  227.880005  229.500000  227.830002  229.289993   \n",
      "2025-02-07 19:30:00+00:00  228.000000  228.580002  227.789993  227.889999   \n",
      "2025-02-07 20:30:00+00:00  227.710007  228.149994  227.259995  227.990005   \n",
      "\n",
      "                           Volume_AAPL        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2025-02-07 16:30:00+00:00      3763438  2025-02-07            0.0  \n",
      "2025-02-07 17:30:00+00:00      3721234  2025-02-07            0.0  \n",
      "2025-02-07 18:30:00+00:00      2602577  2025-02-07            0.0  \n",
      "2025-02-07 19:30:00+00:00      2965606  2025-02-07            0.0  \n",
      "2025-02-07 20:30:00+00:00      5030649  2025-02-07            0.0  \n",
      "Data shape: (2462, 7)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_14736\\2744847720.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved AAPL data with sentiment to stock_data\\AAPL_data_with_sentiment.csv\n",
      "[DEBUG] GOOG stock data shape: (2462, 7), columns: ['Close_GOOG', 'High_GOOG', 'Low_GOOG', 'Open_GOOG', 'Volume_GOOG', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for GOOG in r/wallstreetbets...\n",
      "Fetched 200 posts for GOOG.\n",
      "[DEBUG] sentiment_daily shape: (163, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for GOOG ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2462 entries, 2023-09-01 13:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Close_GOOG     2462 non-null   float64\n",
      " 1   High_GOOG      2462 non-null   float64\n",
      " 2   Low_GOOG       2462 non-null   float64\n",
      " 3   Open_GOOG      2462 non-null   float64\n",
      " 4   Volume_GOOG    2462 non-null   int64  \n",
      " 5   Date           2462 non-null   object \n",
      " 6   avg_sentiment  2462 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 153.9+ KB\n",
      "None\n",
      "                           Close_GOOG   High_GOOG    Low_GOOG   Open_GOOG  \\\n",
      "Datetime                                                                    \n",
      "2023-09-01 13:30:00+00:00  137.160004  138.580002  135.940002  138.429993   \n",
      "2023-09-01 14:30:00+00:00  136.919998  137.160004  136.179993  137.149994   \n",
      "2023-09-01 15:30:00+00:00  136.940002  137.059998  136.419998  136.940002   \n",
      "2023-09-01 16:30:00+00:00  136.509995  136.990005  136.434998  136.929993   \n",
      "2023-09-01 17:30:00+00:00  136.229202  136.649994  136.229202  136.500000   \n",
      "\n",
      "                           Volume_GOOG        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2023-09-01 13:30:00+00:00      5073659  2023-09-01            0.0  \n",
      "2023-09-01 14:30:00+00:00      1968502  2023-09-01            0.0  \n",
      "2023-09-01 15:30:00+00:00      1323053  2023-09-01            0.0  \n",
      "2023-09-01 16:30:00+00:00      1130775  2023-09-01            0.0  \n",
      "2023-09-01 17:30:00+00:00      1030241  2023-09-01            0.0  \n",
      "                           Close_GOOG   High_GOOG    Low_GOOG   Open_GOOG  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  185.445007  187.490005  185.110001  187.347504   \n",
      "2025-02-07 17:30:00+00:00  187.828506  187.839996  185.220001  185.440002   \n",
      "2025-02-07 18:30:00+00:00  187.369995  188.179993  186.869995  187.809998   \n",
      "2025-02-07 19:30:00+00:00  187.123703  187.789993  187.100006  187.369995   \n",
      "2025-02-07 20:30:00+00:00  187.190002  187.460007  186.899994  187.119995   \n",
      "\n",
      "                           Volume_GOOG        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2025-02-07 16:30:00+00:00      3875821  2025-02-07        -0.2411  \n",
      "2025-02-07 17:30:00+00:00      2476479  2025-02-07        -0.2411  \n",
      "2025-02-07 18:30:00+00:00      2290224  2025-02-07        -0.2411  \n",
      "2025-02-07 19:30:00+00:00      2085096  2025-02-07        -0.2411  \n",
      "2025-02-07 20:30:00+00:00      2508441  2025-02-07        -0.2411  \n",
      "Data shape: (2462, 7)\n",
      "\n",
      "Saved GOOG data with sentiment to stock_data\\GOOG_data_with_sentiment.csv\n",
      "[DEBUG] MSFT stock data shape: (2462, 7), columns: ['Close_MSFT', 'High_MSFT', 'Low_MSFT', 'Open_MSFT', 'Volume_MSFT', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for MSFT in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_14736\\2744847720.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 200 posts for MSFT.\n",
      "[DEBUG] sentiment_daily shape: (169, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for MSFT ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2462 entries, 2023-09-01 13:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Close_MSFT     2462 non-null   float64\n",
      " 1   High_MSFT      2462 non-null   float64\n",
      " 2   Low_MSFT       2462 non-null   float64\n",
      " 3   Open_MSFT      2462 non-null   float64\n",
      " 4   Volume_MSFT    2462 non-null   int64  \n",
      " 5   Date           2462 non-null   object \n",
      " 6   avg_sentiment  2462 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 153.9+ KB\n",
      "None\n",
      "                           Close_MSFT   High_MSFT    Low_MSFT   Open_MSFT  \\\n",
      "Datetime                                                                    \n",
      "2023-09-01 13:30:00+00:00  330.089996  331.989990  327.450012  331.309998   \n",
      "2023-09-01 14:30:00+00:00  328.630005  330.070007  327.864990  330.070007   \n",
      "2023-09-01 15:30:00+00:00  328.109985  328.869995  327.480011  328.649994   \n",
      "2023-09-01 16:30:00+00:00  327.320007  328.329987  327.299988  328.095001   \n",
      "2023-09-01 17:30:00+00:00  327.190002  327.909393  326.779999  327.299988   \n",
      "\n",
      "                           Volume_MSFT        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2023-09-01 13:30:00+00:00      3854429  2023-09-01            0.0  \n",
      "2023-09-01 14:30:00+00:00      1997748  2023-09-01            0.0  \n",
      "2023-09-01 15:30:00+00:00      1329892  2023-09-01            0.0  \n",
      "2023-09-01 16:30:00+00:00      1194451  2023-09-01            0.0  \n",
      "2023-09-01 17:30:00+00:00      1071318  2023-09-01            0.0  \n",
      "                           Close_MSFT   High_MSFT    Low_MSFT   Open_MSFT  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  410.180115  411.660004  409.559998  411.359985   \n",
      "2025-02-07 17:30:00+00:00  410.709991  411.339996  410.179993  410.179993   \n",
      "2025-02-07 18:30:00+00:00  408.285004  411.141998  408.279999  410.694397   \n",
      "2025-02-07 19:30:00+00:00  408.875000  409.159912  408.100006  408.269989   \n",
      "2025-02-07 20:30:00+00:00  409.760010  410.350006  408.809998  408.859985   \n",
      "\n",
      "                           Volume_MSFT        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2025-02-07 16:30:00+00:00      2450726  2025-02-07            0.0  \n",
      "2025-02-07 17:30:00+00:00      1559172  2025-02-07            0.0  \n",
      "2025-02-07 18:30:00+00:00      1948018  2025-02-07            0.0  \n",
      "2025-02-07 19:30:00+00:00      1847588  2025-02-07            0.0  \n",
      "2025-02-07 20:30:00+00:00      2749459  2025-02-07            0.0  \n",
      "Data shape: (2462, 7)\n",
      "\n",
      "Saved MSFT data with sentiment to stock_data\\MSFT_data_with_sentiment.csv\n",
      "[DEBUG] AMZN stock data shape: (2462, 7), columns: ['Close_AMZN', 'High_AMZN', 'Low_AMZN', 'Open_AMZN', 'Volume_AMZN', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for AMZN in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_14736\\2744847720.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 200 posts for AMZN.\n",
      "[DEBUG] sentiment_daily shape: (174, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for AMZN ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2462 entries, 2023-09-01 13:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Close_AMZN     2462 non-null   float64\n",
      " 1   High_AMZN      2462 non-null   float64\n",
      " 2   Low_AMZN       2462 non-null   float64\n",
      " 3   Open_AMZN      2462 non-null   float64\n",
      " 4   Volume_AMZN    2462 non-null   int64  \n",
      " 5   Date           2462 non-null   object \n",
      " 6   avg_sentiment  2462 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 153.9+ KB\n",
      "None\n",
      "                           Close_AMZN   High_AMZN    Low_AMZN   Open_AMZN  \\\n",
      "Datetime                                                                    \n",
      "2023-09-01 13:30:00+00:00  138.720001  139.960007  137.929993  139.455002   \n",
      "2023-09-01 14:30:00+00:00  137.869995  138.720001  137.279999  138.690002   \n",
      "2023-09-01 15:30:00+00:00  137.259995  137.990005  136.929993  137.880005   \n",
      "2023-09-01 16:30:00+00:00  136.999893  137.550003  136.875000  137.250000   \n",
      "2023-09-01 17:30:00+00:00  137.130005  137.559998  136.960007  136.985001   \n",
      "\n",
      "                           Volume_AMZN        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2023-09-01 13:30:00+00:00     11264608  2023-09-01            0.0  \n",
      "2023-09-01 14:30:00+00:00      5653789  2023-09-01            0.0  \n",
      "2023-09-01 15:30:00+00:00      4074198  2023-09-01            0.0  \n",
      "2023-09-01 16:30:00+00:00      2949915  2023-09-01            0.0  \n",
      "2023-09-01 17:30:00+00:00      2366914  2023-09-01            0.0  \n",
      "                           Close_AMZN   High_AMZN    Low_AMZN   Open_AMZN  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  229.029999  229.889999  228.070007  229.130005   \n",
      "2025-02-07 17:30:00+00:00  230.330002  230.970001  228.929993  229.009995   \n",
      "2025-02-07 18:30:00+00:00  228.934998  230.990005  228.869995  230.320007   \n",
      "2025-02-07 19:30:00+00:00  228.899994  229.375000  228.630005  228.949997   \n",
      "2025-02-07 20:30:00+00:00  229.270004  230.039993  228.710007  228.884995   \n",
      "\n",
      "                           Volume_AMZN        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2025-02-07 16:30:00+00:00      6746854  2025-02-07         0.1406  \n",
      "2025-02-07 17:30:00+00:00      5357940  2025-02-07         0.1406  \n",
      "2025-02-07 18:30:00+00:00      4588232  2025-02-07         0.1406  \n",
      "2025-02-07 19:30:00+00:00      4026561  2025-02-07         0.1406  \n",
      "2025-02-07 20:30:00+00:00      8612009  2025-02-07         0.1406  \n",
      "Data shape: (2462, 7)\n",
      "\n",
      "Saved AMZN data with sentiment to stock_data\\AMZN_data_with_sentiment.csv\n",
      "[DEBUG] NVDA stock data shape: (2462, 7), columns: ['Close_NVDA', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', 'Volume_NVDA', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for NVDA in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_14736\\2744847720.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 200 posts for NVDA.\n",
      "[DEBUG] sentiment_daily shape: (145, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for NVDA ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2462 entries, 2023-09-01 13:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Close_NVDA     2462 non-null   float64\n",
      " 1   High_NVDA      2462 non-null   float64\n",
      " 2   Low_NVDA       2462 non-null   float64\n",
      " 3   Open_NVDA      2462 non-null   float64\n",
      " 4   Volume_NVDA    2462 non-null   int64  \n",
      " 5   Date           2462 non-null   object \n",
      " 6   avg_sentiment  2462 non-null   float64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 153.9+ KB\n",
      "None\n",
      "                           Close_NVDA   High_NVDA    Low_NVDA   Open_NVDA  \\\n",
      "Datetime                                                                    \n",
      "2023-09-01 13:30:00+00:00  488.019989  498.000000  484.630005  497.619995   \n",
      "2023-09-01 14:30:00+00:00  484.149994  488.239899  481.415985  487.989990   \n",
      "2023-09-01 15:30:00+00:00  484.859985  486.070007  482.130096  484.220001   \n",
      "2023-09-01 16:30:00+00:00  485.760010  487.989899  484.100006  484.850006   \n",
      "2023-09-01 17:30:00+00:00  484.691193  487.249908  484.290100  485.700104   \n",
      "\n",
      "                           Volume_NVDA        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2023-09-01 13:30:00+00:00     14927157  2023-09-01            0.0  \n",
      "2023-09-01 14:30:00+00:00      9336383  2023-09-01            0.0  \n",
      "2023-09-01 15:30:00+00:00      5097873  2023-09-01            0.0  \n",
      "2023-09-01 16:30:00+00:00      4442381  2023-09-01            0.0  \n",
      "2023-09-01 17:30:00+00:00      3375136  2023-09-01            0.0  \n",
      "                           Close_NVDA   High_NVDA    Low_NVDA   Open_NVDA  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  128.235001  129.059601  127.599998  128.699997   \n",
      "2025-02-07 17:30:00+00:00  129.255005  129.649994  128.020004  128.229996   \n",
      "2025-02-07 18:30:00+00:00  128.130005  129.429993  127.908798  129.259903   \n",
      "2025-02-07 19:30:00+00:00  128.948502  129.089905  127.970001  128.139999   \n",
      "2025-02-07 20:30:00+00:00  129.860001  130.000000  128.820007  128.945007   \n",
      "\n",
      "                           Volume_NVDA        Date  avg_sentiment  \n",
      "Datetime                                                           \n",
      "2025-02-07 16:30:00+00:00     27757064  2025-02-07            0.0  \n",
      "2025-02-07 17:30:00+00:00     17653887  2025-02-07            0.0  \n",
      "2025-02-07 18:30:00+00:00     16675456  2025-02-07            0.0  \n",
      "2025-02-07 19:30:00+00:00     16455557  2025-02-07            0.0  \n",
      "2025-02-07 20:30:00+00:00     21996032  2025-02-07            0.0  \n",
      "Data shape: (2462, 7)\n",
      "\n",
      "Saved NVDA data with sentiment to stock_data\\NVDA_data_with_sentiment.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_14736\\2744847720.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ensure the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# --- Reddit API Credentials ---\n",
    "REDDIT_CLIENT_ID = \"VQ-NOvyPWyJvGZs1ifD0Ww\"\n",
    "REDDIT_CLIENT_SECRET = \"BX_Dlp6miv2eMo4qt5JY_imgYVyMBA\"\n",
    "REDDIT_USER_AGENT = \"StockSentimentAnalysis/0.1 by Joseph\"\n",
    "\n",
    "# Initialize the Reddit client (PRAW)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "def fetch_intraday_chunks(ticker, start_date, end_date, interval=\"1h\", max_days=60):\n",
    "    \"\"\"\n",
    "    Fetch intraday data in chunks for a given ticker from start_date to end_date,\n",
    "    avoiding yfinance's ~60-day intraday limit by splitting the date range.\n",
    "    \n",
    "    Returns a single DataFrame for the entire period, with a single-level DatetimeIndex.\n",
    "    \"\"\"\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    all_data = []\n",
    "    current_start = start_dt\n",
    "\n",
    "    while current_start < end_dt:\n",
    "        current_end = current_start + timedelta(days=max_days)\n",
    "        if current_end > end_dt:\n",
    "            current_end = end_dt\n",
    "\n",
    "        chunk_start_str = current_start.strftime(\"%Y-%m-%d\")\n",
    "        chunk_end_str = current_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        print(f\"Fetching {ticker} data from {chunk_start_str} to {chunk_end_str} with {interval} interval...\")\n",
    "        chunk_data = yf.download(ticker, start=chunk_start_str, end=chunk_end_str, interval=interval)\n",
    "        \n",
    "        if not chunk_data.empty:\n",
    "            all_data.append(chunk_data)\n",
    "        else:\n",
    "            print(f\"No data returned for {ticker} from {chunk_start_str} to {chunk_end_str}.\")\n",
    "        \n",
    "        current_start = current_end + timedelta(days=1)\n",
    "    \n",
    "    if all_data:\n",
    "        full_data = pd.concat(all_data)\n",
    "        full_data.sort_index(inplace=True)\n",
    "        \n",
    "        # Ensure index is a proper DatetimeIndex\n",
    "        if not pd.api.types.is_datetime64_any_dtype(full_data.index):\n",
    "            print(\"[DEBUG] Converting index to datetime...\")\n",
    "            full_data.index = pd.to_datetime(full_data.index, errors='coerce')\n",
    "        \n",
    "        # Flatten multi-level columns if needed\n",
    "        if isinstance(full_data.columns, pd.MultiIndex):\n",
    "            full_data.columns = [\n",
    "                \"_\".join(col) if isinstance(col, tuple) else col\n",
    "                for col in full_data.columns\n",
    "            ]\n",
    "        \n",
    "        # Drop top level if we have a multi-level index (e.g., (ticker, datetime))\n",
    "        if full_data.index.nlevels > 1:\n",
    "            print(\"[DEBUG] Dropping the top index level...\")\n",
    "            full_data.index = full_data.index.droplevel(0)\n",
    "        \n",
    "        return full_data\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_stocks_data(ticker_list, start_date, end_date, interval=\"1h\", max_days=60):\n",
    "    \"\"\"\n",
    "    Fetch historical intraday stock data for each ticker over a large date range by chunking.\n",
    "    Returns a dictionary {ticker: DataFrame}.\n",
    "    \"\"\"\n",
    "    stocks_data = {}\n",
    "    for ticker in ticker_list:\n",
    "        data = fetch_intraday_chunks(ticker, start_date, end_date, interval, max_days)\n",
    "        stocks_data[ticker] = data\n",
    "    return stocks_data\n",
    "\n",
    "def fetch_reddit_posts(ticker, limit=200, subreddit=\"wallstreetbets\"):\n",
    "    \"\"\"\n",
    "    Fetch up to 'limit' Reddit posts from a subreddit that mention the ticker.\n",
    "    (PRAW doesn't allow date-based filtering, so we do a broad search and later\n",
    "    aggregate by date in Python.)\n",
    "    \n",
    "    Returns: list of dict, each with {'created': datetime, 'text': ...}\n",
    "    \"\"\"\n",
    "    print(f\"Fetching up to {limit} Reddit posts for {ticker} in r/{subreddit}...\")\n",
    "    posts = []\n",
    "    try:\n",
    "        for submission in reddit.subreddit(subreddit).search(ticker, limit=limit):\n",
    "            created_dt = pd.to_datetime(submission.created_utc, unit='s', utc=True)\n",
    "            created_dt = created_dt.tz_localize(None)\n",
    "            text = f\"{submission.title} {submission.selftext}\"\n",
    "            posts.append({'created': created_dt, 'text': text})\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Reddit posts: {e}\")\n",
    "    print(f\"Fetched {len(posts)} posts for {ticker}.\")\n",
    "    return posts\n",
    "\n",
    "def analyze_sentiment(posts):\n",
    "    \"\"\"\n",
    "    Perform VADER sentiment analysis on each post.\n",
    "    Returns a DataFrame with columns ['created', 'compound'].\n",
    "    \"\"\"\n",
    "    if not posts:\n",
    "        return pd.DataFrame(columns=[\"created\", \"compound\"])\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "    for p in posts:\n",
    "        scores = sid.polarity_scores(p['text'])\n",
    "        results.append({\n",
    "            'created': p['created'],\n",
    "            'compound': scores['compound']\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def aggregate_sentiment_by_date(sentiment_df):\n",
    "    \"\"\"\n",
    "    Convert each post's 'created' datetime to a date, then average the 'compound' score by date.\n",
    "    Returns a DataFrame indexed by 'Date' with column 'avg_sentiment'.\n",
    "    \"\"\"\n",
    "    if sentiment_df.empty:\n",
    "        return pd.DataFrame(columns=[\"avg_sentiment\"])\n",
    "    \n",
    "    sentiment_df['Date'] = sentiment_df['created'].dt.date\n",
    "    grouped = sentiment_df.groupby('Date')['compound'].mean().reset_index()\n",
    "    grouped.rename(columns={'compound': 'avg_sentiment'}, inplace=True)\n",
    "    grouped.set_index('Date', inplace=True)\n",
    "    return grouped\n",
    "\n",
    "def merge_stock_with_sentiment(stock_df, ticker):\n",
    "    \"\"\"\n",
    "    1) Store original intraday index in 'Datetime' column\n",
    "    2) Create a 'Date' column from that intraday index\n",
    "    3) Reset the index to a RangeIndex\n",
    "    4) Fetch & analyze Reddit posts for 'ticker'\n",
    "    5) Aggregate sentiment by date\n",
    "    6) Merge on 'Date'\n",
    "    7) Restore the original intraday index as row labels\n",
    "    8) Return a DataFrame with 'avg_sentiment'\n",
    "    \"\"\"\n",
    "    if stock_df.empty:\n",
    "        return stock_df\n",
    "    \n",
    "    # 1) Store original intraday index in a new column\n",
    "    stock_df['Datetime'] = stock_df.index\n",
    "    \n",
    "    # 2) Create a 'Date' column from that intraday index\n",
    "    stock_df['Date'] = stock_df.index.date\n",
    "    \n",
    "    # 3) Reset the index\n",
    "    stock_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"[DEBUG] {ticker} stock data shape: {stock_df.shape}, columns: {stock_df.columns.tolist()}\")\n",
    "\n",
    "    # 4) Fetch & analyze Reddit posts\n",
    "    posts = fetch_reddit_posts(ticker, limit=200)\n",
    "    sentiment_df = analyze_sentiment(posts)\n",
    "    sentiment_daily = aggregate_sentiment_by_date(sentiment_df)\n",
    "    \n",
    "    print(f\"[DEBUG] sentiment_daily shape: {sentiment_daily.shape}, columns: {sentiment_daily.columns.tolist()}\")\n",
    "    \n",
    "    # 5) Merge on 'Date'\n",
    "    merged_df = stock_df.merge(sentiment_daily, how='left', left_on='Date', right_on='Date')\n",
    "    merged_df['avg_sentiment'].fillna(0, inplace=True)\n",
    "    \n",
    "    # 7) Restore the original intraday timestamps as row labels\n",
    "    #    We'll set the index to 'Datetime'\n",
    "    merged_df.set_index('Datetime', inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def save_stocks_to_csv(stocks_data, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    For each ticker:\n",
    "      1) Merge the chunked stock data with Reddit sentiment\n",
    "      2) Print the final merged DataFrame\n",
    "      3) Save to CSV\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for ticker, data in stocks_data.items():\n",
    "        if data.empty:\n",
    "            print(f\"No data for {ticker}; skipping sentiment merge and CSV save.\")\n",
    "            continue\n",
    "        \n",
    "        merged_df = merge_stock_with_sentiment(data, ticker)\n",
    "        \n",
    "        print(f\"\\n--- Final merged DataFrame for {ticker} ---\")\n",
    "        print(merged_df.info())\n",
    "        print(merged_df.head(5))\n",
    "        print(merged_df.tail(5))\n",
    "        print(f\"Data shape: {merged_df.shape}\\n\")\n",
    "        \n",
    "        file_path = os.path.join(output_dir, f\"{ticker}_data_with_sentiment.csv\")\n",
    "        merged_df.to_csv(file_path)\n",
    "        print(f\"Saved {ticker} data with sentiment to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = [\"AAPL\", \"GOOG\", \"MSFT\", \"AMZN\", \"NVDA\"]\n",
    "    start_date = \"2023-09-01\"\n",
    "    end_date = \"2025-02-10\"\n",
    "    \n",
    "    stocks_data = get_stocks_data(tickers, start_date, end_date, interval=\"1h\", max_days=60)\n",
    "    save_stocks_to_csv(stocks_data, output_dir=\"stock_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
