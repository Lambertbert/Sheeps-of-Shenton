{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3389932788.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install yfinance\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting websocket-client>=0.54.0 (from praw)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.5 MB/s eta 0:00:00\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: websocket-client, tqdm, regex, joblib, click, update_checker, prawcore, nltk, praw\n",
      "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 praw-7.8.1 prawcore-2.4.0 regex-2024.11.6 tqdm-4.67.1 update_checker-0.18.0 websocket-client-1.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install praw nltk pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching AAPL data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching AAPL data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching AAPL data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching AAPL data from 2024-05-02 to 2024-07-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2024-07-02 to 2024-08-31 with 1h interval...\n",
      "Fetching AAPL data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching AAPL data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching AAPL data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Fetching GOOG data from 2023-09-01 to 2023-10-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GOOG data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching GOOG data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching GOOG data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching GOOG data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching GOOG data from 2024-07-02 to 2024-08-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GOOG data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching GOOG data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching GOOG data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Fetching MSFT data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching MSFT data from 2023-11-01 to 2023-12-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-07-02 to 2024-08-31 with 1h interval...\n",
      "Fetching MSFT data from 2024-09-01 to 2024-10-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching MSFT data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Fetching AMZN data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching AMZN data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching AMZN data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching AMZN data from 2024-03-02 to 2024-05-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AMZN data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching AMZN data from 2024-07-02 to 2024-08-31 with 1h interval...\n",
      "Fetching AMZN data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching AMZN data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching AMZN data from 2025-01-01 to 2025-02-10 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NVDA data from 2023-09-01 to 2023-10-31 with 1h interval...\n",
      "Fetching NVDA data from 2023-11-01 to 2023-12-31 with 1h interval...\n",
      "Fetching NVDA data from 2024-01-01 to 2024-03-01 with 1h interval...\n",
      "Fetching NVDA data from 2024-03-02 to 2024-05-01 with 1h interval...\n",
      "Fetching NVDA data from 2024-05-02 to 2024-07-01 with 1h interval...\n",
      "Fetching NVDA data from 2024-07-02 to 2024-08-31 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NVDA data from 2024-09-01 to 2024-10-31 with 1h interval...\n",
      "Fetching NVDA data from 2024-11-01 to 2024-12-31 with 1h interval...\n",
      "Fetching NVDA data from 2025-01-01 to 2025-02-10 with 1h interval...\n",
      "Saved AAPL data to stock_data\\AAPL_data.csv\n",
      "Saved GOOG data to stock_data\\GOOG_data.csv\n",
      "Saved MSFT data to stock_data\\MSFT_data.csv\n",
      "Saved AMZN data to stock_data\\AMZN_data.csv\n",
      "Saved NVDA data to stock_data\\NVDA_data.csv\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_intraday_chunks(ticker, start_date, end_date, interval=\"1h\", max_days=60):\n",
    "    \"\"\"\n",
    "    Fetch intraday data in chunks for a given ticker from start_date to end_date.\n",
    "    \n",
    "    Parameters:\n",
    "        ticker (str): Stock ticker symbol (e.g., \"AAPL\").\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "        interval (str): Data interval (default is \"1h\").\n",
    "        max_days (int): Maximum number of days per chunk.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Concatenated DataFrame with the entire period's data.\n",
    "    \"\"\"\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    all_data = []\n",
    "    current_start = start_dt\n",
    "\n",
    "    while current_start < end_dt:\n",
    "        current_end = current_start + timedelta(days=max_days)\n",
    "        if current_end > end_dt:\n",
    "            current_end = end_dt\n",
    "\n",
    "        # Convert current chunk start and end to string format\n",
    "        chunk_start_str = current_start.strftime(\"%Y-%m-%d\")\n",
    "        chunk_end_str = current_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        print(f\"Fetching {ticker} data from {chunk_start_str} to {chunk_end_str} with {interval} interval...\")\n",
    "        chunk_data = yf.download(ticker, start=chunk_start_str, end=chunk_end_str, interval=interval)\n",
    "        \n",
    "        if not chunk_data.empty:\n",
    "            all_data.append(chunk_data)\n",
    "        else:\n",
    "            print(f\"No data returned for {ticker} from {chunk_start_str} to {chunk_end_str}.\")\n",
    "        \n",
    "        # Move to the next chunk. To avoid duplicate records at boundaries, add one day.\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "    \n",
    "    if all_data:\n",
    "        # Concatenate all chunks and sort by index (date/time)\n",
    "        full_data = pd.concat(all_data)\n",
    "        full_data.sort_index(inplace=True)\n",
    "        return full_data\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_stocks_data(ticker_list, start_date, end_date, interval=\"1h\", max_days=60):\n",
    "    \"\"\"\n",
    "    Fetch historical intraday stock data for each ticker over a large date range by chunking.\n",
    "    \n",
    "    Parameters:\n",
    "        ticker_list (list): List of stock ticker symbols (e.g., [\"AAPL\", \"GOOG\"]).\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "        interval (str): Data interval (default is \"1h\").\n",
    "        max_days (int): Maximum number of days per chunk.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary with ticker symbols as keys and concatenated DataFrames as values.\n",
    "    \"\"\"\n",
    "    stocks_data = {}\n",
    "    for ticker in ticker_list:\n",
    "        data = fetch_intraday_chunks(ticker, start_date, end_date, interval, max_days)\n",
    "        stocks_data[ticker] = data\n",
    "    return stocks_data\n",
    "\n",
    "def save_stocks_to_csv(stocks_data, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Save each ticker's data to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        stocks_data (dict): Dictionary with ticker symbols as keys and DataFrames as values.\n",
    "        output_dir (str): Directory to save CSV files.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for ticker, data in stocks_data.items():\n",
    "        file_path = os.path.join(output_dir, f\"{ticker}_data.csv\")\n",
    "        data.to_csv(file_path)\n",
    "        print(f\"Saved {ticker} data to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of stock tickers to process\n",
    "    tickers = [\"AAPL\", \"GOOG\", \"MSFT\", \"AMZN\", \"NVDA\"]\n",
    "    \n",
    "    # Define a longer time range that spans more than 60 days\n",
    "    start_date = \"2023-09-01\"\n",
    "    end_date = \"2025-02-10\"\n",
    "    \n",
    "    # Fetch data in chunks and then combine\n",
    "    stocks_data = get_stocks_data(tickers, start_date, end_date, interval=\"1h\", max_days=60)\n",
    "    \n",
    "    # Save each stock's combined data into CSV files in the 'stock_data' directory\n",
    "    save_stocks_to_csv(stocks_data, output_dir=\"stock_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\notjo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\notjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_30448\\1488100300.py:90: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_data.index = full_data.index.floor('H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL stock data in chunks from 2023-08-01 to 2023-12-31 at 1h intervals...\n",
      "  Fetching chunk: 2023-08-01 to 2023-09-30\n",
      "  Fetching chunk: 2023-10-01 to 2023-11-30\n",
      "  Fetching chunk: 2023-12-01 to 2023-12-31\n",
      "Fetching Reddit posts for 'AAPL' from r/wallstreetbets...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot join tz-naive with tz-aware DatetimeIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 209\u001b[0m\n\u001b[0;32m    206\u001b[0m SUBREDDIT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwallstreetbets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m POST_LIMIT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m         \u001b[38;5;66;03m# Number of Reddit posts to fetch\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m \u001b[43mrun_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTICKER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTART_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINTERVAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubreddit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSUBREDDIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPOST_LIMIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    217\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 190\u001b[0m, in \u001b[0;36mrun_analysis\u001b[1;34m(ticker, start_date, end_date, interval, subreddit, limit, output_dir)\u001b[0m\n\u001b[0;32m    187\u001b[0m aggregated_sentiment \u001b[38;5;241m=\u001b[39m aggregate_sentiment_by_hour(sentiment_df)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Step 6: Merge with stock data\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_stock_and_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregated_sentiment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Step 7: Save final CSV\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir):\n",
      "Cell \u001b[1;32mIn[7], line 160\u001b[0m, in \u001b[0;36mmerge_stock_and_sentiment\u001b[1;34m(stock_df, sentiment_df)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStock DataFrame is empty, skipping merge.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stock_df\n\u001b[1;32m--> 160\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mstock_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentiment_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merged_df\n",
      "File \u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:886\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1137\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m right_ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_index \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masof\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1137\u001b[0m     join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[43mleft_ax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_ax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_indexers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1142\u001b[0m     join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1143\u001b[0m         left_ax, right_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1144\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:279\u001b[0m, in \u001b[0;36m_maybe_return_indexers.<locals>.join\u001b[1;34m(self, other, how, level, return_indexers, sort)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(meth)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    278\u001b[0m ):\n\u001b[1;32m--> 279\u001b[0m     join_index, lidx, ridx \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_indexers:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m join_index\n",
      "File \u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4598\u001b[0m, in \u001b[0;36mIndex.join\u001b[1;34m(self, other, how, level, return_indexers, sort)\u001b[0m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDatetimeIndex) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, ABCDatetimeIndex):\n\u001b[0;32m   4596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m^\u001b[39m (other\u001b[38;5;241m.\u001b[39mtz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   4597\u001b[0m         \u001b[38;5;66;03m# Raise instead of casting to object below.\u001b[39;00m\n\u001b[1;32m-> 4598\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot join tz-naive with tz-aware DatetimeIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m other\u001b[38;5;241m.\u001b[39m_is_multi:\n\u001b[0;32m   4601\u001b[0m     \u001b[38;5;66;03m# We have specific handling for MultiIndex below\u001b[39;00m\n\u001b[0;32m   4602\u001b[0m     pself, pother \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_downcast_for_indexing(other)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot join tz-naive with tz-aware DatetimeIndex"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Ensure VADER is available\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# --- Reddit API Credentials ---\n",
    "REDDIT_CLIENT_ID = \"VQ-NOvyPWyJvGZs1ifD0Ww\"\n",
    "REDDIT_CLIENT_SECRET = \"BX_Dlp6miv2eMo4qt5JY_imgYVyMBA\"\n",
    "REDDIT_USER_AGENT = \"StockSentimentAnalysis/0.1 by Joseph\"\n",
    "\n",
    "# Initialize PRAW (Reddit client)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "def fetch_stock_data_in_chunks(ticker, start_date, end_date, interval=\"1h\", chunk_size=60):\n",
    "    \"\"\"\n",
    "    Fetch stock data from Yahoo Finance in chunks to bypass the ~60-day intraday limit.\n",
    "    \n",
    "    Parameters:\n",
    "        ticker (str): Stock ticker symbol (e.g., \"AAPL\"). \n",
    "                      Pass as a string (not a list) for a single ticker to avoid multi-level columns.\n",
    "        start_date (str): Start date in \"YYYY-MM-DD\" format.\n",
    "        end_date (str): End date in \"YYYY-MM-DD\" format.\n",
    "        interval (str): Data interval (e.g., \"1h\", \"1d\").\n",
    "        chunk_size (int): Number of days per chunk (default=60).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame of all chunks, indexed by floored hour.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching {ticker} stock data in chunks from {start_date} to {end_date} at {interval} intervals...\")\n",
    "\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    \n",
    "    all_chunks = []\n",
    "    current_start = start_dt\n",
    "    \n",
    "    while current_start < end_dt:\n",
    "        # Calculate chunk end\n",
    "        current_end = current_start + timedelta(days=chunk_size)\n",
    "        if current_end > end_dt:\n",
    "            current_end = end_dt\n",
    "        \n",
    "        print(f\"  Fetching chunk: {current_start.date()} to {current_end.date()}\")\n",
    "        df_chunk = yf.download(\n",
    "            ticker,\n",
    "            start=current_start.strftime(\"%Y-%m-%d\"),\n",
    "            end=current_end.strftime(\"%Y-%m-%d\"),\n",
    "            interval=interval\n",
    "        )\n",
    "        \n",
    "        if not df_chunk.empty:\n",
    "            all_chunks.append(df_chunk)\n",
    "        else:\n",
    "            print(\"  No data returned for this chunk.\")\n",
    "        \n",
    "        # Move start to the next day after current_end\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "    \n",
    "    if not all_chunks:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    full_data = pd.concat(all_chunks)\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # FIX: If there's a multi-level index (e.g. (ticker, date)),\n",
    "    # drop the top level so we have a single-level DatetimeIndex.\n",
    "    # ---------------------------------------------------------\n",
    "    if full_data.index.nlevels > 1:\n",
    "        # e.g. drop the first level (the ticker level)\n",
    "        full_data.index = full_data.index.droplevel(0)\n",
    "    \n",
    "    # Also flatten multi-level columns if needed\n",
    "    # (happens if yfinance returns e.g. ('Open', 'AAPL'), etc.)\n",
    "    if isinstance(full_data.columns, pd.MultiIndex):\n",
    "        full_data.columns = ['_'.join(col) if isinstance(col, tuple) else col\n",
    "                             for col in full_data.columns]\n",
    "    \n",
    "    # Floor the index to the hour\n",
    "    full_data.index = full_data.index.floor('H')\n",
    "    \n",
    "    # Drop any duplicate timestamps if they exist\n",
    "    full_data = full_data[~full_data.index.duplicated(keep='first')]\n",
    "    \n",
    "    return full_data\n",
    "\n",
    "def fetch_reddit_posts(stock_symbol, subreddit=\"wallstreetbets\", limit=100):\n",
    "    \"\"\"\n",
    "    Fetch posts from a given subreddit that mention the stock symbol.\n",
    "    \n",
    "    Returns:\n",
    "        list of dict: Each dict has:\n",
    "            - 'created': datetime (floored to the hour)\n",
    "            - 'text': combined title + selftext\n",
    "    \"\"\"\n",
    "    print(f\"Fetching Reddit posts for '{stock_symbol}' from r/{subreddit}...\")\n",
    "    posts = []\n",
    "    try:\n",
    "        for submission in reddit.subreddit(subreddit).search(stock_symbol, limit=limit):\n",
    "            # Convert UTC timestamp to datetime and floor to the hour\n",
    "            created_dt = datetime.fromtimestamp(submission.created_utc).replace(\n",
    "                minute=0, second=0, microsecond=0\n",
    "            )\n",
    "            text = f\"{submission.title} {submission.selftext}\"\n",
    "            posts.append({'created': created_dt, 'text': text})\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Reddit posts: {e}\")\n",
    "    return posts\n",
    "\n",
    "def analyze_posts_sentiment(posts):\n",
    "    \"\"\"\n",
    "    Analyze sentiment for each post using VADER.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 'created' (datetime) and 'compound' (VADER compound score).\n",
    "    \"\"\"\n",
    "    if not posts:\n",
    "        return pd.DataFrame(columns=['created', 'compound'])\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "    for post in posts:\n",
    "        scores = analyzer.polarity_scores(post['text'])\n",
    "        results.append({'created': post['created'], 'compound': scores['compound']})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def aggregate_sentiment_by_hour(sentiment_df):\n",
    "    \"\"\"\n",
    "    Aggregates sentiment scores by hour (average compound score).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Indexed by 'created' (hour) with column 'avg_sentiment'.\n",
    "    \"\"\"\n",
    "    if sentiment_df.empty:\n",
    "        return pd.DataFrame(columns=['avg_sentiment'])\n",
    "    \n",
    "    grouped = sentiment_df.groupby('created')['compound'].mean().reset_index()\n",
    "    grouped.rename(columns={'compound': 'avg_sentiment'}, inplace=True)\n",
    "    grouped.set_index('created', inplace=True)\n",
    "    return grouped\n",
    "\n",
    "def merge_stock_and_sentiment(stock_df, sentiment_df):\n",
    "    \"\"\"\n",
    "    Merges the sentiment data (avg_sentiment) with the stock DataFrame on the hourly index.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with a new 'avg_sentiment' column.\n",
    "    \"\"\"\n",
    "    if stock_df.empty:\n",
    "        print(\"Stock DataFrame is empty, skipping merge.\")\n",
    "        return stock_df\n",
    "    \n",
    "    merged_df = stock_df.merge(sentiment_df, how='left', left_index=True, right_index=True)\n",
    "    merged_df['avg_sentiment'].fillna(0, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "def run_analysis(ticker, start_date, end_date, interval=\"1h\", subreddit=\"wallstreetbets\", limit=100, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    1) Fetch stock data in chunks (bypass 60-day limit)\n",
    "    2) Fix multi-level index if present\n",
    "    3) Fetch Reddit posts\n",
    "    4) Analyze sentiment\n",
    "    5) Aggregate by hour\n",
    "    6) Merge with stock data\n",
    "    7) Save final CSV\n",
    "    \"\"\"\n",
    "    # Step 1 & 2: Fetch stock data in chunks and fix multi-level index\n",
    "    stock_df = fetch_stock_data_in_chunks(ticker, start_date, end_date, interval=interval, chunk_size=60)\n",
    "    if stock_df.empty:\n",
    "        print(f\"No stock data found for {ticker} in the given date range.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Fetch Reddit posts\n",
    "    posts = fetch_reddit_posts(ticker, subreddit=subreddit, limit=limit)\n",
    "    \n",
    "    # Step 4: Analyze sentiment\n",
    "    sentiment_df = analyze_posts_sentiment(posts)\n",
    "    \n",
    "    # Step 5: Aggregate sentiment by hour\n",
    "    aggregated_sentiment = aggregate_sentiment_by_hour(sentiment_df)\n",
    "    \n",
    "    # Step 6: Merge with stock data\n",
    "    merged_df = merge_stock_and_sentiment(stock_df, aggregated_sentiment)\n",
    "    \n",
    "    # Step 7: Save final CSV\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{ticker}_merged_data.csv\")\n",
    "    merged_df.to_csv(output_path)\n",
    "    print(f\"Final merged CSV saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    TICKER = \"AAPL\"          # Single ticker as a string\n",
    "    START_DATE = \"2023-08-01\"\n",
    "    END_DATE = \"2023-12-31\"\n",
    "    INTERVAL = \"1h\"          # '1d' for daily, '1h' for hourly, etc.\n",
    "    SUBREDDIT = \"wallstreetbets\"\n",
    "    POST_LIMIT = 100         # Number of Reddit posts to fetch\n",
    "    \n",
    "    run_analysis(\n",
    "        ticker=TICKER,\n",
    "        start_date=START_DATE,\n",
    "        end_date=END_DATE,\n",
    "        interval=INTERVAL,\n",
    "        subreddit=SUBREDDIT,\n",
    "        limit=POST_LIMIT,\n",
    "        output_dir=\"output\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
