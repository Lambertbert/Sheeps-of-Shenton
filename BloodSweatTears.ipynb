{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.54)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from yfinance) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yfinance) (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (7.8.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: nltk in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: click in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\notjo\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\notjo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "pip install praw nltk pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\notjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\notjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2023-01-01 to 2023-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['AAPL']: YFPricesMissingError('possibly delisted; no price data found  (1h 2023-01-01 -> 2023-03-02) (Yahoo error = \"1h data not available for startTime=1672549200 and endTime=1677733200. The requested range must be within the last 730 days.\")')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned for AAPL from 2023-01-01 to 2023-03-02.\n",
      "Fetching AAPL data from 2023-03-03 to 2023-05-02 with 1h interval...\n",
      "Fetching AAPL data from 2023-05-03 to 2023-07-02 with 1h interval...\n",
      "Fetching AAPL data from 2023-07-03 to 2023-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2023-09-02 to 2023-11-01 with 1h interval...\n",
      "Fetching AAPL data from 2023-11-02 to 2024-01-01 with 1h interval...\n",
      "Fetching AAPL data from 2024-01-02 to 2024-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2024-03-03 to 2024-05-02 with 1h interval...\n",
      "Fetching AAPL data from 2024-05-03 to 2024-07-02 with 1h interval...\n",
      "Fetching AAPL data from 2024-07-03 to 2024-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AAPL data from 2024-09-02 to 2024-11-01 with 1h interval...\n",
      "Fetching AAPL data from 2024-11-02 to 2025-01-01 with 1h interval...\n",
      "Fetching AAPL data from 2025-01-02 to 2025-02-10 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Technical indicator columns added: ['Close', 'High_AAPL', 'Low_AAPL', 'Open_AAPL', 'Volume_AAPL', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram']\n",
      "Fetching GOOG data from 2023-01-01 to 2023-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['GOOG']: YFPricesMissingError('possibly delisted; no price data found  (1h 2023-01-01 -> 2023-03-02) (Yahoo error = \"1h data not available for startTime=1672549200 and endTime=1677733200. The requested range must be within the last 730 days.\")')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned for GOOG from 2023-01-01 to 2023-03-02.\n",
      "Fetching GOOG data from 2023-03-03 to 2023-05-02 with 1h interval...\n",
      "Fetching GOOG data from 2023-05-03 to 2023-07-02 with 1h interval...\n",
      "Fetching GOOG data from 2023-07-03 to 2023-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GOOG data from 2023-09-02 to 2023-11-01 with 1h interval...\n",
      "Fetching GOOG data from 2023-11-02 to 2024-01-01 with 1h interval...\n",
      "Fetching GOOG data from 2024-01-02 to 2024-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GOOG data from 2024-03-03 to 2024-05-02 with 1h interval...\n",
      "Fetching GOOG data from 2024-05-03 to 2024-07-02 with 1h interval...\n",
      "Fetching GOOG data from 2024-07-03 to 2024-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching GOOG data from 2024-09-02 to 2024-11-01 with 1h interval...\n",
      "Fetching GOOG data from 2024-11-02 to 2025-01-01 with 1h interval...\n",
      "Fetching GOOG data from 2025-01-02 to 2025-02-10 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Technical indicator columns added: ['Close', 'High_GOOG', 'Low_GOOG', 'Open_GOOG', 'Volume_GOOG', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram']\n",
      "Fetching MSFT data from 2023-01-01 to 2023-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['MSFT']: YFPricesMissingError('possibly delisted; no price data found  (1h 2023-01-01 -> 2023-03-02) (Yahoo error = \"1h data not available for startTime=1672549200 and endTime=1677733200. The requested range must be within the last 730 days.\")')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned for MSFT from 2023-01-01 to 2023-03-02.\n",
      "Fetching MSFT data from 2023-03-03 to 2023-05-02 with 1h interval...\n",
      "Fetching MSFT data from 2023-05-03 to 2023-07-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2023-07-03 to 2023-09-01 with 1h interval...\n",
      "Fetching MSFT data from 2023-09-02 to 2023-11-01 with 1h interval...\n",
      "Fetching MSFT data from 2023-11-02 to 2024-01-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2024-01-02 to 2024-03-02 with 1h interval...\n",
      "Fetching MSFT data from 2024-03-03 to 2024-05-02 with 1h interval...\n",
      "Fetching MSFT data from 2024-05-03 to 2024-07-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2024-07-03 to 2024-09-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-09-02 to 2024-11-01 with 1h interval...\n",
      "Fetching MSFT data from 2024-11-02 to 2025-01-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching MSFT data from 2025-01-02 to 2025-02-10 with 1h interval...\n",
      "[DEBUG] Technical indicator columns added: ['Close', 'High_MSFT', 'Low_MSFT', 'Open_MSFT', 'Volume_MSFT', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram']\n",
      "Fetching AMZN data from 2023-01-01 to 2023-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['AMZN']: YFPricesMissingError('possibly delisted; no price data found  (1h 2023-01-01 -> 2023-03-02) (Yahoo error = \"1h data not available for startTime=1672549200 and endTime=1677733200. The requested range must be within the last 730 days.\")')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned for AMZN from 2023-01-01 to 2023-03-02.\n",
      "Fetching AMZN data from 2023-03-03 to 2023-05-02 with 1h interval...\n",
      "Fetching AMZN data from 2023-05-03 to 2023-07-02 with 1h interval...\n",
      "Fetching AMZN data from 2023-07-03 to 2023-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AMZN data from 2023-09-02 to 2023-11-01 with 1h interval...\n",
      "Fetching AMZN data from 2023-11-02 to 2024-01-01 with 1h interval...\n",
      "Fetching AMZN data from 2024-01-02 to 2024-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AMZN data from 2024-03-03 to 2024-05-02 with 1h interval...\n",
      "Fetching AMZN data from 2024-05-03 to 2024-07-02 with 1h interval...\n",
      "Fetching AMZN data from 2024-07-03 to 2024-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AMZN data from 2024-09-02 to 2024-11-01 with 1h interval...\n",
      "Fetching AMZN data from 2024-11-02 to 2025-01-01 with 1h interval...\n",
      "Fetching AMZN data from 2025-01-02 to 2025-02-10 with 1h interval...\n",
      "[DEBUG] Technical indicator columns added: ['Close', 'High_AMZN', 'Low_AMZN', 'Open_AMZN', 'Volume_AMZN', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram']\n",
      "Fetching NVDA data from 2023-01-01 to 2023-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['NVDA']: YFPricesMissingError('possibly delisted; no price data found  (1h 2023-01-01 -> 2023-03-02) (Yahoo error = \"1h data not available for startTime=1672549200 and endTime=1677733200. The requested range must be within the last 730 days.\")')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data returned for NVDA from 2023-01-01 to 2023-03-02.\n",
      "Fetching NVDA data from 2023-03-03 to 2023-05-02 with 1h interval...\n",
      "Fetching NVDA data from 2023-05-03 to 2023-07-02 with 1h interval...\n",
      "Fetching NVDA data from 2023-07-03 to 2023-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NVDA data from 2023-09-02 to 2023-11-01 with 1h interval...\n",
      "Fetching NVDA data from 2023-11-02 to 2024-01-01 with 1h interval...\n",
      "Fetching NVDA data from 2024-01-02 to 2024-03-02 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NVDA data from 2024-03-03 to 2024-05-02 with 1h interval...\n",
      "Fetching NVDA data from 2024-05-03 to 2024-07-02 with 1h interval...\n",
      "Fetching NVDA data from 2024-07-03 to 2024-09-01 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching NVDA data from 2024-09-02 to 2024-11-01 with 1h interval...\n",
      "Fetching NVDA data from 2024-11-02 to 2025-01-01 with 1h interval...\n",
      "Fetching NVDA data from 2025-01-02 to 2025-02-10 with 1h interval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Technical indicator columns added: ['Close', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', 'Volume_NVDA', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram']\n",
      "[DEBUG] AAPL stock data shape: (3340, 19), columns: ['Close', 'High_AAPL', 'Low_AAPL', 'Open_AAPL', 'Volume_AAPL', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'Hour', 'HourlyCandleCount', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for AAPL in r/wallstreetbets...\n",
      "Fetched 200 posts for AAPL.\n",
      "[DEBUG] sentiment_daily shape: (177, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for AAPL ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3340 entries, 2023-03-03 14:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Close              3340 non-null   float64\n",
      " 1   High_AAPL          3340 non-null   float64\n",
      " 2   Low_AAPL           3340 non-null   float64\n",
      " 3   Open_AAPL          3340 non-null   float64\n",
      " 4   Volume_AAPL        3340 non-null   int64  \n",
      " 5   SMA20              3321 non-null   float64\n",
      " 6   SMA44              3297 non-null   float64\n",
      " 7   SMA50              3291 non-null   float64\n",
      " 8   SMA100             3241 non-null   float64\n",
      " 9   SMA200             3141 non-null   float64\n",
      " 10  EMA12              3340 non-null   float64\n",
      " 11  EMA26              3340 non-null   float64\n",
      " 12  MACD               3340 non-null   float64\n",
      " 13  MACD_Signal        3340 non-null   float64\n",
      " 14  MACD_Histogram     3340 non-null   float64\n",
      " 15  Hour               3340 non-null   int32  \n",
      " 16  HourlyCandleCount  3340 non-null   int64  \n",
      " 17  Date               3340 non-null   object \n",
      " 18  avg_sentiment      3340 non-null   float64\n",
      "dtypes: float64(15), int32(1), int64(2), object(1)\n",
      "memory usage: 508.8+ KB\n",
      "None\n",
      "                                Close   High_AAPL    Low_AAPL   Open_AAPL  \\\n",
      "Datetime                                                                    \n",
      "2023-03-03 14:30:00+00:00  150.149994  150.160004  147.449997  147.629898   \n",
      "2023-03-03 15:30:00+00:00  149.570007  150.169998  149.375000  150.139999   \n",
      "2023-03-03 16:30:00+00:00  149.914200  150.029999  149.259995  149.574997   \n",
      "2023-03-03 17:30:00+00:00  150.101395  150.320007  149.600098  149.914993   \n",
      "2023-03-03 18:30:00+00:00  150.759995  150.809998  149.910004  150.104996   \n",
      "\n",
      "                           Volume_AAPL  SMA20  SMA44  SMA50  SMA100  SMA200  \\\n",
      "Datetime                                                                      \n",
      "2023-03-03 14:30:00+00:00     18325087    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 15:30:00+00:00      7392346    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 16:30:00+00:00      6663071    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 17:30:00+00:00      6588836    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 18:30:00+00:00      5696300    NaN    NaN    NaN     NaN     NaN   \n",
      "\n",
      "                                EMA12       EMA26      MACD  MACD_Signal  \\\n",
      "Datetime                                                                   \n",
      "2023-03-03 14:30:00+00:00  150.149994  150.149994  0.000000     0.000000   \n",
      "2023-03-03 15:30:00+00:00  150.060765  150.107032 -0.046267    -0.009253   \n",
      "2023-03-03 16:30:00+00:00  150.038217  150.092748 -0.054531    -0.018309   \n",
      "2023-03-03 17:30:00+00:00  150.047936  150.093389 -0.045452    -0.023738   \n",
      "2023-03-03 18:30:00+00:00  150.157484  150.142767  0.014717    -0.016047   \n",
      "\n",
      "                           MACD_Histogram  Hour  HourlyCandleCount  \\\n",
      "Datetime                                                             \n",
      "2023-03-03 14:30:00+00:00        0.000000    14                  1   \n",
      "2023-03-03 15:30:00+00:00       -0.037013    15                  2   \n",
      "2023-03-03 16:30:00+00:00       -0.036222    16                  3   \n",
      "2023-03-03 17:30:00+00:00       -0.021715    17                  4   \n",
      "2023-03-03 18:30:00+00:00        0.030764    18                  5   \n",
      "\n",
      "                                 Date  avg_sentiment  \n",
      "Datetime                                              \n",
      "2023-03-03 14:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 15:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 16:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 17:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 18:30:00+00:00  2023-03-03            0.0  \n",
      "                                Close   High_AAPL    Low_AAPL   Open_AAPL  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  228.400604  229.419998  228.195007  229.085007   \n",
      "2025-02-07 17:30:00+00:00  229.298904  229.440002  228.363297  228.389999   \n",
      "2025-02-07 18:30:00+00:00  227.880005  229.500000  227.830002  229.289993   \n",
      "2025-02-07 19:30:00+00:00  228.000000  228.580002  227.789993  227.889999   \n",
      "2025-02-07 20:30:00+00:00  227.710007  228.149994  227.259995  227.990005   \n",
      "\n",
      "                           Volume_AAPL       SMA20       SMA44       SMA50  \\\n",
      "Datetime                                                                     \n",
      "2025-02-07 16:30:00+00:00      3763438  231.256165  232.656043  233.289398   \n",
      "2025-02-07 17:30:00+00:00      3721234  231.114110  232.454654  233.137996   \n",
      "2025-02-07 18:30:00+00:00      2602577  230.892660  232.223972  232.946996   \n",
      "2025-02-07 19:30:00+00:00      2965606  230.653160  231.986472  232.778096   \n",
      "2025-02-07 20:30:00+00:00      5030649  230.563660  231.724200  232.551296   \n",
      "\n",
      "                               SMA100      SMA200       EMA12       EMA26  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  230.302418  236.753509  230.873983  231.398037   \n",
      "2025-02-07 17:30:00+00:00  230.282594  236.613203  230.631663  231.242546   \n",
      "2025-02-07 18:30:00+00:00  230.258803  236.460797  230.208331  230.993468   \n",
      "2025-02-07 19:30:00+00:00  230.239703  236.306397  229.868588  230.771730   \n",
      "2025-02-07 20:30:00+00:00  230.225004  236.150023  229.536498  230.544936   \n",
      "\n",
      "                               MACD  MACD_Signal  MACD_Histogram  Hour  \\\n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00 -0.524054    -0.339880       -0.184174    16   \n",
      "2025-02-07 17:30:00+00:00 -0.610882    -0.394081       -0.216802    17   \n",
      "2025-02-07 18:30:00+00:00 -0.785137    -0.472292       -0.312845    18   \n",
      "2025-02-07 19:30:00+00:00 -0.903142    -0.558462       -0.344680    19   \n",
      "2025-02-07 20:30:00+00:00 -1.008437    -0.648457       -0.359980    20   \n",
      "\n",
      "                           HourlyCandleCount        Date  avg_sentiment  \n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00                  3  2025-02-07            0.0  \n",
      "2025-02-07 17:30:00+00:00                  4  2025-02-07            0.0  \n",
      "2025-02-07 18:30:00+00:00                  5  2025-02-07            0.0  \n",
      "2025-02-07 19:30:00+00:00                  6  2025-02-07            0.0  \n",
      "2025-02-07 20:30:00+00:00                  7  2025-02-07            0.0  \n",
      "Data shape: (3340, 19)\n",
      "\n",
      "Saved AAPL data with sentiment to stock_data\\AAPL_data_with_sentiment.csv\n",
      "[DEBUG] GOOG stock data shape: (3340, 19), columns: ['Close', 'High_GOOG', 'Low_GOOG', 'Open_GOOG', 'Volume_GOOG', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'Hour', 'HourlyCandleCount', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for GOOG in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_31716\\1367220173.py:231: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 200 posts for GOOG.\n",
      "[DEBUG] sentiment_daily shape: (163, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for GOOG ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3340 entries, 2023-03-03 14:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Close              3340 non-null   float64\n",
      " 1   High_GOOG          3340 non-null   float64\n",
      " 2   Low_GOOG           3340 non-null   float64\n",
      " 3   Open_GOOG          3340 non-null   float64\n",
      " 4   Volume_GOOG        3340 non-null   int64  \n",
      " 5   SMA20              3321 non-null   float64\n",
      " 6   SMA44              3297 non-null   float64\n",
      " 7   SMA50              3291 non-null   float64\n",
      " 8   SMA100             3241 non-null   float64\n",
      " 9   SMA200             3141 non-null   float64\n",
      " 10  EMA12              3340 non-null   float64\n",
      " 11  EMA26              3340 non-null   float64\n",
      " 12  MACD               3340 non-null   float64\n",
      " 13  MACD_Signal        3340 non-null   float64\n",
      " 14  MACD_Histogram     3340 non-null   float64\n",
      " 15  Hour               3340 non-null   int32  \n",
      " 16  HourlyCandleCount  3340 non-null   int64  \n",
      " 17  Date               3340 non-null   object \n",
      " 18  avg_sentiment      3340 non-null   float64\n",
      "dtypes: float64(15), int32(1), int64(2), object(1)\n",
      "memory usage: 508.8+ KB\n",
      "None\n",
      "                               Close  High_GOOG   Low_GOOG  Open_GOOG  \\\n",
      "Datetime                                                                \n",
      "2023-03-03 14:30:00+00:00  93.650002  93.875000  92.660004  92.739998   \n",
      "2023-03-03 15:30:00+00:00  93.519997  93.900002  93.370003  93.650002   \n",
      "2023-03-03 16:30:00+00:00  93.669998  93.790001  93.370003  93.510002   \n",
      "2023-03-03 17:30:00+00:00  93.629997  93.785004  93.459999  93.660004   \n",
      "2023-03-03 18:30:00+00:00  93.900002  93.959999  93.550003  93.629997   \n",
      "\n",
      "                           Volume_GOOG  SMA20  SMA44  SMA50  SMA100  SMA200  \\\n",
      "Datetime                                                                      \n",
      "2023-03-03 14:30:00+00:00      6560170    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 15:30:00+00:00      3932657    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 16:30:00+00:00      2563078    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 17:30:00+00:00      2797435    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 18:30:00+00:00      2561836    NaN    NaN    NaN     NaN     NaN   \n",
      "\n",
      "                               EMA12      EMA26      MACD  MACD_Signal  \\\n",
      "Datetime                                                                 \n",
      "2023-03-03 14:30:00+00:00  93.650002  93.650002  0.000000     0.000000   \n",
      "2023-03-03 15:30:00+00:00  93.630001  93.640372 -0.010371    -0.002074   \n",
      "2023-03-03 16:30:00+00:00  93.636154  93.642566 -0.006412    -0.002942   \n",
      "2023-03-03 17:30:00+00:00  93.635207  93.641635 -0.006428    -0.003639   \n",
      "2023-03-03 18:30:00+00:00  93.675945  93.660773  0.015171     0.000123   \n",
      "\n",
      "                           MACD_Histogram  Hour  HourlyCandleCount  \\\n",
      "Datetime                                                             \n",
      "2023-03-03 14:30:00+00:00        0.000000    14                  1   \n",
      "2023-03-03 15:30:00+00:00       -0.008297    15                  2   \n",
      "2023-03-03 16:30:00+00:00       -0.003470    16                  3   \n",
      "2023-03-03 17:30:00+00:00       -0.002789    17                  4   \n",
      "2023-03-03 18:30:00+00:00        0.015048    18                  5   \n",
      "\n",
      "                                 Date  avg_sentiment  \n",
      "Datetime                                              \n",
      "2023-03-03 14:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 15:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 16:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 17:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 18:30:00+00:00  2023-03-03            0.0  \n",
      "                                Close   High_GOOG    Low_GOOG   Open_GOOG  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  185.445007  187.490005  185.110001  187.347504   \n",
      "2025-02-07 17:30:00+00:00  187.828506  187.839996  185.220001  185.440002   \n",
      "2025-02-07 18:30:00+00:00  187.369995  188.179993  186.869995  187.809998   \n",
      "2025-02-07 19:30:00+00:00  187.123703  187.789993  187.100006  187.369995   \n",
      "2025-02-07 20:30:00+00:00  187.190002  187.460007  186.899994  187.119995   \n",
      "\n",
      "                           Volume_GOOG       SMA20       SMA44       SMA50  \\\n",
      "Datetime                                                                     \n",
      "2025-02-07 16:30:00+00:00      3875821  193.806129  199.645465  199.439779   \n",
      "2025-02-07 17:30:00+00:00      2476479  192.851805  199.339294  199.258149   \n",
      "2025-02-07 18:30:00+00:00      2290224  191.866305  199.027931  199.069649   \n",
      "2025-02-07 19:30:00+00:00      2085096  190.834490  198.702105  198.872923   \n",
      "2025-02-07 20:30:00+00:00      2508441  190.680490  198.365060  198.653923   \n",
      "\n",
      "                               SMA100      SMA200       EMA12       EMA26  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  199.133849  196.719006  191.183341  194.642940   \n",
      "2025-02-07 17:30:00+00:00  199.035134  196.674799  190.667213  194.138167   \n",
      "2025-02-07 18:30:00+00:00  198.932634  196.623049  190.159948  193.636821   \n",
      "2025-02-07 19:30:00+00:00  198.820972  196.571467  189.692834  193.154368   \n",
      "2025-02-07 20:30:00+00:00  198.717272  196.520888  189.307783  192.712563   \n",
      "\n",
      "                               MACD  MACD_Signal  MACD_Histogram  Hour  \\\n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00 -3.459599    -2.862576       -0.597023    16   \n",
      "2025-02-07 17:30:00+00:00 -3.470954    -2.984251       -0.486703    17   \n",
      "2025-02-07 18:30:00+00:00 -3.476872    -3.082776       -0.394097    18   \n",
      "2025-02-07 19:30:00+00:00 -3.461534    -3.158527       -0.303007    19   \n",
      "2025-02-07 20:30:00+00:00 -3.404780    -3.207778       -0.197002    20   \n",
      "\n",
      "                           HourlyCandleCount        Date  avg_sentiment  \n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00                  3  2025-02-07        -0.2411  \n",
      "2025-02-07 17:30:00+00:00                  4  2025-02-07        -0.2411  \n",
      "2025-02-07 18:30:00+00:00                  5  2025-02-07        -0.2411  \n",
      "2025-02-07 19:30:00+00:00                  6  2025-02-07        -0.2411  \n",
      "2025-02-07 20:30:00+00:00                  7  2025-02-07        -0.2411  \n",
      "Data shape: (3340, 19)\n",
      "\n",
      "Saved GOOG data with sentiment to stock_data\\GOOG_data_with_sentiment.csv\n",
      "[DEBUG] MSFT stock data shape: (3340, 19), columns: ['Close', 'High_MSFT', 'Low_MSFT', 'Open_MSFT', 'Volume_MSFT', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'Hour', 'HourlyCandleCount', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for MSFT in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_31716\\1367220173.py:231: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 200 posts for MSFT.\n",
      "[DEBUG] sentiment_daily shape: (169, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for MSFT ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3340 entries, 2023-03-03 14:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Close              3340 non-null   float64\n",
      " 1   High_MSFT          3340 non-null   float64\n",
      " 2   Low_MSFT           3340 non-null   float64\n",
      " 3   Open_MSFT          3340 non-null   float64\n",
      " 4   Volume_MSFT        3340 non-null   int64  \n",
      " 5   SMA20              3321 non-null   float64\n",
      " 6   SMA44              3297 non-null   float64\n",
      " 7   SMA50              3291 non-null   float64\n",
      " 8   SMA100             3241 non-null   float64\n",
      " 9   SMA200             3141 non-null   float64\n",
      " 10  EMA12              3340 non-null   float64\n",
      " 11  EMA26              3340 non-null   float64\n",
      " 12  MACD               3340 non-null   float64\n",
      " 13  MACD_Signal        3340 non-null   float64\n",
      " 14  MACD_Histogram     3340 non-null   float64\n",
      " 15  Hour               3340 non-null   int32  \n",
      " 16  HourlyCandleCount  3340 non-null   int64  \n",
      " 17  Date               3340 non-null   object \n",
      " 18  avg_sentiment      3340 non-null   float64\n",
      "dtypes: float64(15), int32(1), int64(2), object(1)\n",
      "memory usage: 508.8+ KB\n",
      "None\n",
      "                                Close   High_MSFT    Low_MSFT   Open_MSFT  \\\n",
      "Datetime                                                                    \n",
      "2023-03-03 14:30:00+00:00  255.100006  255.250000  251.389999  251.880005   \n",
      "2023-03-03 15:30:00+00:00  253.744995  255.130005  253.330002  255.100006   \n",
      "2023-03-03 16:30:00+00:00  253.940002  254.029999  253.250000  253.759995   \n",
      "2023-03-03 17:30:00+00:00  255.050003  255.419998  253.690002  253.940002   \n",
      "2023-03-03 18:30:00+00:00  255.399902  255.449997  254.644302  255.050003   \n",
      "\n",
      "                           Volume_MSFT  SMA20  SMA44  SMA50  SMA100  SMA200  \\\n",
      "Datetime                                                                      \n",
      "2023-03-03 14:30:00+00:00      5476810    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 15:30:00+00:00      2540013    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 16:30:00+00:00      2202392    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 17:30:00+00:00      1858632    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 18:30:00+00:00      1526905    NaN    NaN    NaN     NaN     NaN   \n",
      "\n",
      "                                EMA12       EMA26      MACD  MACD_Signal  \\\n",
      "Datetime                                                                   \n",
      "2023-03-03 14:30:00+00:00  255.100006  255.100006  0.000000     0.000000   \n",
      "2023-03-03 15:30:00+00:00  254.891543  254.999635 -0.108092    -0.021618   \n",
      "2023-03-03 16:30:00+00:00  254.745152  254.921144 -0.175992    -0.052493   \n",
      "2023-03-03 17:30:00+00:00  254.792052  254.930689 -0.138637    -0.069722   \n",
      "2023-03-03 18:30:00+00:00  254.885568  254.965445 -0.079878    -0.071753   \n",
      "\n",
      "                           MACD_Histogram  Hour  HourlyCandleCount  \\\n",
      "Datetime                                                             \n",
      "2023-03-03 14:30:00+00:00        0.000000    14                  1   \n",
      "2023-03-03 15:30:00+00:00       -0.086474    15                  2   \n",
      "2023-03-03 16:30:00+00:00       -0.123499    16                  3   \n",
      "2023-03-03 17:30:00+00:00       -0.068915    17                  4   \n",
      "2023-03-03 18:30:00+00:00       -0.008125    18                  5   \n",
      "\n",
      "                                 Date  avg_sentiment  \n",
      "Datetime                                              \n",
      "2023-03-03 14:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 15:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 16:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 17:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 18:30:00+00:00  2023-03-03            0.0  \n",
      "                                Close   High_MSFT    Low_MSFT   Open_MSFT  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  410.180115  411.660004  409.559998  411.359985   \n",
      "2025-02-07 17:30:00+00:00  410.709991  411.339996  410.179993  410.179993   \n",
      "2025-02-07 18:30:00+00:00  408.285004  411.141998  408.279999  410.694397   \n",
      "2025-02-07 19:30:00+00:00  408.875000  409.159912  408.100006  408.269989   \n",
      "2025-02-07 20:30:00+00:00  409.760010  410.350006  408.809998  408.859985   \n",
      "\n",
      "                           Volume_MSFT       SMA20       SMA44       SMA50  \\\n",
      "Datetime                                                                     \n",
      "2025-02-07 16:30:00+00:00      2450726  413.646146  414.058350  416.984360   \n",
      "2025-02-07 17:30:00+00:00      1559172  413.556755  413.955168  416.326756   \n",
      "2025-02-07 18:30:00+00:00      1948018  413.373006  413.813816  415.632456   \n",
      "2025-02-07 19:30:00+00:00      1847588  413.201256  413.658248  414.968656   \n",
      "2025-02-07 20:30:00+00:00      2749459  413.113756  413.534385  414.264346   \n",
      "\n",
      "                               SMA100      SMA200       EMA12       EMA26  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  427.764086  426.064061  413.531966  414.595230   \n",
      "2025-02-07 17:30:00+00:00  427.572236  425.924961  413.097816  414.307435   \n",
      "2025-02-07 18:30:00+00:00  427.348586  425.775432  412.357384  413.861329   \n",
      "2025-02-07 19:30:00+00:00  427.117498  425.625857  411.821632  413.491971   \n",
      "2025-02-07 20:30:00+00:00  426.914074  425.479688  411.504460  413.215529   \n",
      "\n",
      "                               MACD  MACD_Signal  MACD_Histogram  Hour  \\\n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00 -1.063264    -1.028782       -0.034482    16   \n",
      "2025-02-07 17:30:00+00:00 -1.209618    -1.064949       -0.144669    17   \n",
      "2025-02-07 18:30:00+00:00 -1.503945    -1.152749       -0.351196    18   \n",
      "2025-02-07 19:30:00+00:00 -1.670339    -1.256267       -0.414072    19   \n",
      "2025-02-07 20:30:00+00:00 -1.711070    -1.347227       -0.363843    20   \n",
      "\n",
      "                           HourlyCandleCount        Date  avg_sentiment  \n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00                  3  2025-02-07            0.0  \n",
      "2025-02-07 17:30:00+00:00                  4  2025-02-07            0.0  \n",
      "2025-02-07 18:30:00+00:00                  5  2025-02-07            0.0  \n",
      "2025-02-07 19:30:00+00:00                  6  2025-02-07            0.0  \n",
      "2025-02-07 20:30:00+00:00                  7  2025-02-07            0.0  \n",
      "Data shape: (3340, 19)\n",
      "\n",
      "Saved MSFT data with sentiment to stock_data\\MSFT_data_with_sentiment.csv\n",
      "[DEBUG] AMZN stock data shape: (3340, 19), columns: ['Close', 'High_AMZN', 'Low_AMZN', 'Open_AMZN', 'Volume_AMZN', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'Hour', 'HourlyCandleCount', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for AMZN in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_31716\\1367220173.py:231: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 200 posts for AMZN.\n",
      "[DEBUG] sentiment_daily shape: (174, 1), columns: ['avg_sentiment']\n",
      "\n",
      "--- Final merged DataFrame for AMZN ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3340 entries, 2023-03-03 14:30:00+00:00 to 2025-02-07 20:30:00+00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Close              3340 non-null   float64\n",
      " 1   High_AMZN          3340 non-null   float64\n",
      " 2   Low_AMZN           3340 non-null   float64\n",
      " 3   Open_AMZN          3340 non-null   float64\n",
      " 4   Volume_AMZN        3340 non-null   int64  \n",
      " 5   SMA20              3321 non-null   float64\n",
      " 6   SMA44              3297 non-null   float64\n",
      " 7   SMA50              3291 non-null   float64\n",
      " 8   SMA100             3241 non-null   float64\n",
      " 9   SMA200             3141 non-null   float64\n",
      " 10  EMA12              3340 non-null   float64\n",
      " 11  EMA26              3340 non-null   float64\n",
      " 12  MACD               3340 non-null   float64\n",
      " 13  MACD_Signal        3340 non-null   float64\n",
      " 14  MACD_Histogram     3340 non-null   float64\n",
      " 15  Hour               3340 non-null   int32  \n",
      " 16  HourlyCandleCount  3340 non-null   int64  \n",
      " 17  Date               3340 non-null   object \n",
      " 18  avg_sentiment      3340 non-null   float64\n",
      "dtypes: float64(15), int32(1), int64(2), object(1)\n",
      "memory usage: 508.8+ KB\n",
      "None\n",
      "                               Close  High_AMZN   Low_AMZN  Open_AMZN  \\\n",
      "Datetime                                                                \n",
      "2023-03-03 14:30:00+00:00  94.714996  94.730003  92.660004  92.739998   \n",
      "2023-03-03 15:30:00+00:00  94.089996  94.750000  93.760002  94.714996   \n",
      "2023-03-03 16:30:00+00:00  94.110001  94.279999  93.910004  94.089996   \n",
      "2023-03-03 17:30:00+00:00  94.213600  94.400002  94.040001  94.110001   \n",
      "2023-03-03 18:30:00+00:00  94.683998  94.699997  94.050003  94.214996   \n",
      "\n",
      "                           Volume_AMZN  SMA20  SMA44  SMA50  SMA100  SMA200  \\\n",
      "Datetime                                                                      \n",
      "2023-03-03 14:30:00+00:00     16066850    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 15:30:00+00:00      6859828    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 16:30:00+00:00      5303826    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 17:30:00+00:00      3947668    NaN    NaN    NaN     NaN     NaN   \n",
      "2023-03-03 18:30:00+00:00      4071756    NaN    NaN    NaN     NaN     NaN   \n",
      "\n",
      "                               EMA12      EMA26      MACD  MACD_Signal  \\\n",
      "Datetime                                                                 \n",
      "2023-03-03 14:30:00+00:00  94.714996  94.714996  0.000000     0.000000   \n",
      "2023-03-03 15:30:00+00:00  94.618842  94.668700 -0.049858    -0.009972   \n",
      "2023-03-03 16:30:00+00:00  94.540559  94.627315 -0.086756    -0.025328   \n",
      "2023-03-03 17:30:00+00:00  94.490258  94.596669 -0.106412    -0.041545   \n",
      "2023-03-03 18:30:00+00:00  94.520064  94.603138 -0.083074    -0.049851   \n",
      "\n",
      "                           MACD_Histogram  Hour  HourlyCandleCount  \\\n",
      "Datetime                                                             \n",
      "2023-03-03 14:30:00+00:00        0.000000    14                  1   \n",
      "2023-03-03 15:30:00+00:00       -0.039886    15                  2   \n",
      "2023-03-03 16:30:00+00:00       -0.061427    16                  3   \n",
      "2023-03-03 17:30:00+00:00       -0.064867    17                  4   \n",
      "2023-03-03 18:30:00+00:00       -0.033223    18                  5   \n",
      "\n",
      "                                 Date  avg_sentiment  \n",
      "Datetime                                              \n",
      "2023-03-03 14:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 15:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 16:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 17:30:00+00:00  2023-03-03            0.0  \n",
      "2023-03-03 18:30:00+00:00  2023-03-03            0.0  \n",
      "                                Close   High_AMZN    Low_AMZN   Open_AMZN  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  229.029999  229.889999  228.070007  229.130005   \n",
      "2025-02-07 17:30:00+00:00  230.330002  230.970001  228.929993  229.009995   \n",
      "2025-02-07 18:30:00+00:00  228.934998  230.990005  228.869995  230.320007   \n",
      "2025-02-07 19:30:00+00:00  228.899994  229.375000  228.630005  228.949997   \n",
      "2025-02-07 20:30:00+00:00  229.270004  230.039993  228.710007  228.884995   \n",
      "\n",
      "                           Volume_AMZN       SMA20       SMA44       SMA50  \\\n",
      "Datetime                                                                     \n",
      "2025-02-07 16:30:00+00:00      6746854  236.744360  237.321269  237.306529   \n",
      "2025-02-07 17:30:00+00:00      5357940  236.153111  237.238541  237.143617   \n",
      "2025-02-07 18:30:00+00:00      4588232  235.511111  237.115246  236.969817   \n",
      "2025-02-07 19:30:00+00:00      4026561  234.851611  236.982973  236.813616   \n",
      "2025-02-07 20:30:00+00:00      8612009  234.449586  236.852746  236.639417   \n",
      "\n",
      "                               SMA100      SMA200       EMA12       EMA26  \\\n",
      "Datetime                                                                    \n",
      "2025-02-07 16:30:00+00:00  235.315634  228.720552  234.504342  236.191753   \n",
      "2025-02-07 17:30:00+00:00  235.365234  228.730127  233.862136  235.757549   \n",
      "2025-02-07 18:30:00+00:00  235.406584  228.735322  233.104114  235.252175   \n",
      "2025-02-07 19:30:00+00:00  235.439077  228.744447  232.457327  234.781643   \n",
      "2025-02-07 20:30:00+00:00  235.475027  228.753197  231.966969  234.373374   \n",
      "\n",
      "                               MACD  MACD_Signal  MACD_Histogram  Hour  \\\n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00 -1.687411    -0.671447       -1.015965    16   \n",
      "2025-02-07 17:30:00+00:00 -1.895414    -0.916240       -0.979174    17   \n",
      "2025-02-07 18:30:00+00:00 -2.148061    -1.162604       -0.985456    18   \n",
      "2025-02-07 19:30:00+00:00 -2.324317    -1.394947       -0.929370    19   \n",
      "2025-02-07 20:30:00+00:00 -2.406404    -1.597238       -0.809166    20   \n",
      "\n",
      "                           HourlyCandleCount        Date  avg_sentiment  \n",
      "Datetime                                                                 \n",
      "2025-02-07 16:30:00+00:00                  3  2025-02-07         0.1406  \n",
      "2025-02-07 17:30:00+00:00                  4  2025-02-07         0.1406  \n",
      "2025-02-07 18:30:00+00:00                  5  2025-02-07         0.1406  \n",
      "2025-02-07 19:30:00+00:00                  6  2025-02-07         0.1406  \n",
      "2025-02-07 20:30:00+00:00                  7  2025-02-07         0.1406  \n",
      "Data shape: (3340, 19)\n",
      "\n",
      "Saved AMZN data with sentiment to stock_data\\AMZN_data_with_sentiment.csv\n",
      "[DEBUG] NVDA stock data shape: (3340, 19), columns: ['Close', 'High_NVDA', 'Low_NVDA', 'Open_NVDA', 'Volume_NVDA', 'SMA20', 'SMA44', 'SMA50', 'SMA100', 'SMA200', 'EMA12', 'EMA26', 'MACD', 'MACD_Signal', 'MACD_Histogram', 'Hour', 'HourlyCandleCount', 'Datetime', 'Date']\n",
      "Fetching up to 200 Reddit posts for NVDA in r/wallstreetbets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\notjo\\AppData\\Local\\Temp\\ipykernel_31716\\1367220173.py:231: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['avg_sentiment'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 0 posts for NVDA.\n",
      "[DEBUG] sentiment_daily shape: (0, 1), columns: ['avg_sentiment']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31716\\1367220173.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mstart_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"2023-01-01\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mend_date\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"2025-02-10\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[0mstocks_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stocks_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"1h\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_days\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m     \u001b[0msave_stocks_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstocks_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"stock_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31716\\1367220173.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(stocks_data, output_dir)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mNo data for \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m; skipping sentiment merge and CSV save.\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_stock_with_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m\\n--- Final merged DataFrame for \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mticker\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m ---\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31716\\1367220173.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(stock_df, ticker)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m[DEBUG] sentiment_daily shape: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msentiment_daily\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m, columns: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0msentiment_daily\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;31m# 5) Merge on 'Date'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mmerged_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment_daily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'avg_sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;31m# 7) Restore the original intraday timestamps as row labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m                             \u001b[1;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\notjo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import praw\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ensure the VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# --- Reddit API Credentials ---\n",
    "REDDIT_CLIENT_ID = \"VQ-NOvyPWyJvGZs1ifD0Ww\"\n",
    "REDDIT_CLIENT_SECRET = \"BX_Dlp6miv2eMo4qt5JY_imgYVyMBA\"\n",
    "REDDIT_USER_AGENT = \"StockSentimentAnalysis/0.1 by Joseph\"\n",
    "\n",
    "# Initialize the Reddit client (PRAW)\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "def fetch_intraday_chunks(ticker, start_date, end_date, interval=\"1h\", max_days=60):\n",
    "    \"\"\"\n",
    "    Fetch intraday data in chunks for a given ticker from start_date to end_date,\n",
    "    avoiding yfinance's ~60-day intraday limit by splitting the date range.\n",
    "    \n",
    "    Returns a single DataFrame for the entire period, with a single-level DatetimeIndex.\n",
    "    \"\"\"\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    all_data = []\n",
    "    current_start = start_dt\n",
    "\n",
    "    while current_start < end_dt:\n",
    "        current_end = current_start + timedelta(days=max_days)\n",
    "        if current_end > end_dt:\n",
    "            current_end = end_dt\n",
    "\n",
    "        chunk_start_str = current_start.strftime(\"%Y-%m-%d\")\n",
    "        chunk_end_str = current_end.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        print(f\"Fetching {ticker} data from {chunk_start_str} to {chunk_end_str} with {interval} interval...\")\n",
    "        chunk_data = yf.download(ticker, start=chunk_start_str, end=chunk_end_str, interval=interval)\n",
    "        \n",
    "        if not chunk_data.empty:\n",
    "            all_data.append(chunk_data)\n",
    "        else:\n",
    "            print(f\"No data returned for {ticker} from {chunk_start_str} to {chunk_end_str}.\")\n",
    "        \n",
    "        current_start = current_end + timedelta(days=1)\n",
    "    \n",
    "    if all_data:\n",
    "        full_data = pd.concat(all_data)\n",
    "        full_data.sort_index(inplace=True)\n",
    "        \n",
    "        # Ensure index is a proper DatetimeIndex\n",
    "        if not pd.api.types.is_datetime64_any_dtype(full_data.index):\n",
    "            print(\"[DEBUG] Converting index to datetime...\")\n",
    "            full_data.index = pd.to_datetime(full_data.index, errors='coerce')\n",
    "        \n",
    "        # Flatten multi-level columns if needed\n",
    "        if isinstance(full_data.columns, pd.MultiIndex):\n",
    "            full_data.columns = [\n",
    "                \"_\".join(col) if isinstance(col, tuple) else col\n",
    "                for col in full_data.columns\n",
    "            ]\n",
    "        \n",
    "        # Drop top level if we have a multi-level index (e.g., (ticker, datetime))\n",
    "        if full_data.index.nlevels > 1:\n",
    "            print(\"[DEBUG] Dropping the top index level...\")\n",
    "            full_data.index = full_data.index.droplevel(0)\n",
    "        \n",
    "        return full_data\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "def add_time_features(data):\n",
    "    \"\"\"\n",
    "    Add time-based features to the DataFrame:\n",
    "      - 'Hour': extracts the hour from the index.\n",
    "      - 'HourlyCandleCount': counts the candle number within each day.\n",
    "    \"\"\"\n",
    "    if not data.empty:\n",
    "        # Extract hour from the DatetimeIndex\n",
    "        data['Hour'] = data.index.hour\n",
    "        # Count the hourly candle for each day (starting at 1)\n",
    "        data['HourlyCandleCount'] = data.groupby(data.index.date).cumcount() + 1\n",
    "    return data\n",
    "    \n",
    "def add_indicators(data, sma_windows=[20, 44, 50, 100, 200]):\n",
    "    \"\"\"\n",
    "    Calculate technical indicators and add them as new columns to the DataFrame.\n",
    "    \n",
    "    Calculates:\n",
    "      - SMA (Simple Moving Average) for each window in sma_windows.\n",
    "      - MACD components (MACD line, Signal line, and Histogram).\n",
    "      \n",
    "    Parameters:\n",
    "        data (DataFrame): DataFrame containing stock data.\n",
    "        sma_windows (list): List of window periods for SMA calculation.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with additional indicator columns.\n",
    "    \"\"\"\n",
    "    data.rename(columns=lambda x: \"Close\" if \"Close\" in x else x, inplace=True)\n",
    "    if not data.empty and \"Close\" in data.columns:\n",
    "        # Calculate SMAs for each specified window\n",
    "        for window in sma_windows:\n",
    "            data[f'SMA{window}'] = data['Close'].rolling(window=window).mean()\n",
    "        \n",
    "        # Calculate MACD components:\n",
    "        # 12-period EMA and 26-period EMA\n",
    "        data['EMA12'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "        data['EMA26'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "        # MACD Line\n",
    "        data['MACD'] = data['EMA12'] - data['EMA26']\n",
    "        # Signal Line: 9-period EMA of MACD\n",
    "        data['MACD_Signal'] = data['MACD'].ewm(span=9, adjust=False).mean()\n",
    "        # Histogram: MACD - Signal Line\n",
    "        data['MACD_Histogram'] = data['MACD'] - data['MACD_Signal']\n",
    "        \n",
    "        # Debug: print the columns to verify technical indicators are added\n",
    "        print(\"[DEBUG] Technical indicator columns added:\", data.columns.tolist())\n",
    "    return data\n",
    "\n",
    "def get_stocks_data(ticker_list, start_date, end_date, interval=\"1h\", max_days=60):\n",
    "    \"\"\"\n",
    "    Fetch historical intraday stock data for each ticker over a large date range by chunking.\n",
    "    Returns a dictionary {ticker: DataFrame}.\n",
    "    \"\"\"\n",
    "    stocks_data = {}\n",
    "    for ticker in ticker_list:\n",
    "        data = fetch_intraday_chunks(ticker, start_date, end_date, interval, max_days)\n",
    "        # Add SMA and MACD technical indicators\n",
    "        data = add_indicators(data, sma_windows=[20, 44, 50, 100, 200])\n",
    "        # Add time features\n",
    "        data = add_time_features(data)\n",
    "        stocks_data[ticker] = data\n",
    "    return stocks_data\n",
    "\n",
    "def fetch_reddit_posts(ticker, limit=200, subreddit=\"wallstreetbets\"):\n",
    "    \"\"\"\n",
    "    Fetch up to 'limit' Reddit posts from a subreddit that mention the ticker.\n",
    "    (PRAW doesn't allow date-based filtering, so we do a broad search and later\n",
    "    aggregate by date in Python.)\n",
    "    \n",
    "    Returns: list of dict, each with {'created': datetime, 'text': ...}\n",
    "    \"\"\"\n",
    "    print(f\"Fetching up to {limit} Reddit posts for {ticker} in r/{subreddit}...\")\n",
    "    posts = []\n",
    "    try:\n",
    "        for submission in reddit.subreddit(subreddit).search(ticker, limit=limit):\n",
    "            created_dt = pd.to_datetime(submission.created_utc, unit='s', utc=True)\n",
    "            created_dt = created_dt.tz_localize(None)\n",
    "            text = f\"{submission.title} {submission.selftext}\"\n",
    "            posts.append({'created': created_dt, 'text': text})\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Reddit posts: {e}\")\n",
    "    print(f\"Fetched {len(posts)} posts for {ticker}.\")\n",
    "    return posts\n",
    "\n",
    "def analyze_sentiment(posts):\n",
    "    \"\"\"\n",
    "    Perform VADER sentiment analysis on each post.\n",
    "    Returns a DataFrame with columns ['created', 'compound'].\n",
    "    \"\"\"\n",
    "    if not posts:\n",
    "        return pd.DataFrame(columns=[\"created\", \"compound\"])\n",
    "    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "    for p in posts:\n",
    "        scores = sid.polarity_scores(p['text'])\n",
    "        results.append({\n",
    "            'created': p['created'],\n",
    "            'compound': scores['compound']\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def aggregate_sentiment_by_date(sentiment_df):\n",
    "    \"\"\"\n",
    "    Convert each post's 'created' datetime to a date, then average the 'compound' score by date.\n",
    "    Returns a DataFrame indexed by 'Date' with column 'avg_sentiment'.\n",
    "    \"\"\"\n",
    "    if sentiment_df.empty:\n",
    "        return pd.DataFrame(columns=[\"avg_sentiment\"])\n",
    "    \n",
    "    sentiment_df['Date'] = sentiment_df['created'].dt.date\n",
    "    grouped = sentiment_df.groupby('Date')['compound'].mean().reset_index()\n",
    "    grouped.rename(columns={'compound': 'avg_sentiment'}, inplace=True)\n",
    "    grouped.set_index('Date', inplace=True)\n",
    "    return grouped\n",
    "\n",
    "def merge_stock_with_sentiment(stock_df, ticker):\n",
    "    \"\"\"\n",
    "    1) Store original intraday index in 'Datetime' column\n",
    "    2) Create a 'Date' column from that intraday index\n",
    "    3) Reset the index to a RangeIndex\n",
    "    4) Fetch & analyze Reddit posts for 'ticker'\n",
    "    5) Aggregate sentiment by date\n",
    "    6) Merge on 'Date'\n",
    "    7) Restore the original intraday index as row labels\n",
    "    8) Return a DataFrame with 'avg_sentiment'\n",
    "    \"\"\"\n",
    "    if stock_df.empty:\n",
    "        return stock_df\n",
    "    \n",
    "    # 1) Store original intraday index in a new column\n",
    "    stock_df['Datetime'] = stock_df.index\n",
    "    \n",
    "    # 2) Create a 'Date' column from that intraday index\n",
    "    stock_df['Date'] = stock_df.index.date\n",
    "    \n",
    "    # 3) Reset the index\n",
    "    stock_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"[DEBUG] {ticker} stock data shape: {stock_df.shape}, columns: {stock_df.columns.tolist()}\")\n",
    "\n",
    "    # 4) Fetch & analyze Reddit posts\n",
    "    posts = fetch_reddit_posts(ticker, limit=200)\n",
    "    sentiment_df = analyze_sentiment(posts)\n",
    "    sentiment_daily = aggregate_sentiment_by_date(sentiment_df)\n",
    "    \n",
    "    print(f\"[DEBUG] sentiment_daily shape: {sentiment_daily.shape}, columns: {sentiment_daily.columns.tolist()}\")\n",
    "    \n",
    "    # 5) Merge on 'Date'\n",
    "    merged_df = stock_df.merge(sentiment_daily, how='left', left_on='Date', right_on='Date')\n",
    "    merged_df['avg_sentiment'].fillna(0, inplace=True)\n",
    "    \n",
    "    # 7) Restore the original intraday timestamps as row labels\n",
    "    #    We'll set the index to 'Datetime'\n",
    "    merged_df.set_index('Datetime', inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def save_stocks_to_csv(stocks_data, output_dir=\".\"):\n",
    "    \"\"\"\n",
    "    For each ticker:\n",
    "      1) Merge the chunked stock data with Reddit sentiment\n",
    "      2) Print the final merged DataFrame\n",
    "      3) Save to CSV\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for ticker, data in stocks_data.items():\n",
    "        if data.empty:\n",
    "            print(f\"No data for {ticker}; skipping sentiment merge and CSV save.\")\n",
    "            continue\n",
    "        \n",
    "        merged_df = merge_stock_with_sentiment(data, ticker)\n",
    "        \n",
    "        print(f\"\\n--- Final merged DataFrame for {ticker} ---\")\n",
    "        print(merged_df.info())\n",
    "        print(merged_df.head(5))\n",
    "        print(merged_df.tail(5))\n",
    "        print(f\"Data shape: {merged_df.shape}\\n\")\n",
    "        \n",
    "        file_path = os.path.join(output_dir, f\"{ticker}_data_with_sentiment.csv\")\n",
    "        merged_df.to_csv(file_path)\n",
    "        print(f\"Saved {ticker} data with sentiment to {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = [\"AAPL\", \"GOOG\", \"MSFT\", \"AMZN\", \"NVDA\"]\n",
    "    start_date = \"2023-01-01\"\n",
    "    end_date = \"2025-02-10\"\n",
    "    \n",
    "    stocks_data = get_stocks_data(tickers, start_date, end_date, interval=\"1h\", max_days=60)\n",
    "    save_stocks_to_csv(stocks_data, output_dir=\"stock_data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
