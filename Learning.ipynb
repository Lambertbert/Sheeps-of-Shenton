{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy matplotlib scikit-learn tensorflow xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the CSV files\n",
    "folder_path = './stock_data'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize a list to store individual DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('combined_data.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been combined into combined_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# ==========================\n",
    "# Configuration Parameters\n",
    "# ==========================\n",
    "csv_path = 'combined_data.csv'  # Update with your CSV file path\n",
    "target_col = 'Close'            # Update with the appropriate column name from your CSV\n",
    "window_size = 30                # Number of time steps in each sample\n",
    "debug_mode = False              # If True, use a small subset (e.g., 20 samples) to debug/overfit\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "max_epochs = 100\n",
    "\n",
    "# ==========================\n",
    "# 1. Load and Preprocess Data\n",
    "# ==========================\n",
    "df = pd.read_csv(csv_path, parse_dates=['Date'])\n",
    "print(\"Shape of df after reading CSV:\", df.shape)\n",
    "print(\"Columns in CSV:\", df.columns.tolist())\n",
    "print(\"Data sample:\\n\", df.head())\n",
    "\n",
    "if df.empty:\n",
    "    raise ValueError(\"The CSV file is empty. Please check the file content or path.\")\n",
    "\n",
    "df.sort_values('Date', inplace=True)\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensure target_col exists in the DataFrame\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(f\"The column '{target_col}' was not found in the CSV file. Check header names.\")\n",
    "\n",
    "# ==========================\n",
    "# 2. Create Labels Based on Data\n",
    "# ==========================\n",
    "# Compute change from previous row and create a binary label (1 for profit, 0 for loss/no gain)\n",
    "df['Change'] = df[target_col] - df[target_col].shift(1)\n",
    "df['Pct_Change'] = df[target_col].pct_change() * 100\n",
    "df['Label'] = (df['Change'] > 0).astype(int)\n",
    "\n",
    "# Only drop rows missing data in the target and 'Change' columns.\n",
    "df.dropna(subset=[target_col, 'Change'], inplace=True)\n",
    "\n",
    "print(\"After dropna, shape of df:\", df.shape)\n",
    "print(\"Label distribution:\\n\", df['Label'].value_counts())\n",
    "\n",
    "# ==========================\n",
    "# 3. Scale the Feature\n",
    "# ==========================\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[[target_col]])\n",
    "if scaled_data.shape[0] == 0:\n",
    "    raise ValueError(\"Scaled data is empty. Check that the target column has data.\")\n",
    "df[f'{target_col}_Scaled'] = scaled_data\n",
    "\n",
    "# ==========================\n",
    "# 4. Create Sliding Window Dataset\n",
    "# ==========================\n",
    "def create_dataset(data, labels, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(labels[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_all, y_all = create_dataset(df[f'{target_col}_Scaled'].values, df['Label'].values, window_size)\n",
    "print(f\"Dataset shapes -> X: {X_all.shape}, y: {y_all.shape}\")\n",
    "\n",
    "if debug_mode:\n",
    "    n_debug = min(20, len(X_all))\n",
    "    X_all, y_all = X_all[:n_debug], y_all[:n_debug]\n",
    "    print(\"Debug mode enabled: Using a small subset for training.\")\n",
    "\n",
    "# ==========================\n",
    "# 5. Split the Data\n",
    "# ==========================\n",
    "# For time series, split chronologically: training on all but the most recent sample.\n",
    "X_train, y_train = X_all[:-1], y_all[:-1]\n",
    "X_test, y_test = X_all[-1:], y_all[-1:]\n",
    "print(f\"Training shapes -> X: {X_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"Test shapes -> X: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# ==========================\n",
    "# 6. Build the Model\n",
    "# ==========================\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', return_sequences=True, input_shape=(window_size, 1)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ==========================\n",
    "# 7. Callbacks and Training\n",
    "# ==========================\n",
    "early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=max_epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8. Evaluation and Prediction\n",
    "# ==========================\n",
    "prediction_prob = model.predict(X_test)[0, 0]\n",
    "prediction_label = 1 if prediction_prob > 0.5 else 0\n",
    "\n",
    "print(f\"\\nPrediction probability for profit on {df.index[-1].date()}: {prediction_prob:.2f}\")\n",
    "print(\"Predicted:\", \"Profit\" if prediction_label == 1 else \"Loss/No Gain\")\n",
    "print(\"Actual label:\", \"Profit\" if y_test[0] == 1 else \"Loss/No Gain\")\n",
    "\n",
    "# ==========================\n",
    "# 9. Plot Training Metrics\n",
    "# ==========================\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy', color='orange')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Prediction from Another File\n",
    "# --------------------------\n",
    "# Specify the path for the new CSV file to predict on.\n",
    "new_csv_path = './stock_data/AAPL_data_with_sentiment.csv'  # Replace with your file path\n",
    "\n",
    "# Load the new data\n",
    "df_new = pd.read_csv(new_csv_path, parse_dates=['Date'])\n",
    "print(\"Shape of new data:\", df_new.shape)\n",
    "print(\"New data sample:\\n\", df_new.head())\n",
    "\n",
    "# Sort and set Date as index\n",
    "df_new.sort_values('Date', inplace=True)\n",
    "df_new.set_index('Date', inplace=True)\n",
    "\n",
    "# IMPORTANT: Ensure the new data has the target column (\"Close\")\n",
    "if target_col not in df_new.columns:\n",
    "    raise KeyError(f\"The column '{target_col}' was not found in the new CSV file.\")\n",
    "\n",
    "# Preprocess the new data\n",
    "# If the new file has extra missing values in columns you care about, you might want to drop those rows.\n",
    "# Here, we assume at least the target column exists and is valid.\n",
    "df_new.dropna(subset=[target_col], inplace=True)\n",
    "\n",
    "# Scale the target column using the same scaler that was fitted on training data.\n",
    "# (Assuming 'scaler' is still in memory from the training code.)\n",
    "df_new[f'{target_col}_Scaled'] = scaler.transform(df_new[[target_col]])\n",
    "\n",
    "# Specify the prediction date in the new data\n",
    "# (Make sure the date exists in the new file.)\n",
    "prediction_date_str = \"2024-11-10\"  # Replace with the desired prediction date\n",
    "prediction_date = pd.to_datetime(prediction_date_str)\n",
    "\n",
    "# Select all data up to and including the prediction date\n",
    "df_new_until_pred = df_new.loc[:prediction_date]\n",
    "\n",
    "if len(df_new_until_pred) < window_size:\n",
    "    raise ValueError(f\"Not enough data to form a window of size {window_size} for prediction on {prediction_date_str}\")\n",
    "\n",
    "# Create the input sample: take the last 'window_size' rows from the new data\n",
    "X_new = df_new_until_pred.tail(window_size)[f'{target_col}_Scaled'].values\n",
    "X_new = X_new.reshape((1, window_size, 1))  # Reshape to 3D for LSTM\n",
    "\n",
    "# Use the trained model to predict\n",
    "new_pred_prob = model.predict(X_new)[0, 0]\n",
    "new_pred_label = 1 if new_pred_prob > 0.5 else 0\n",
    "\n",
    "print(f\"\\nPrediction for {prediction_date_str} from new file:\")\n",
    "print(f\"Prediction probability for profit: {new_pred_prob:.2f}\")\n",
    "print(\"Predicted:\", \"Profit\" if new_pred_label == 1 else \"Loss/No Gain\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
